{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step1: 对于文本的读取以及预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 读取text文档\n",
    "f = open('peotryFromTang.txt',encoding = 'gbk')\n",
    "lines = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 预处理一：去除字符串中间的'\\n',合并同一首诗的字符串\n",
    "peotry = []\n",
    "item = ''\n",
    "for i in range(len(lines)):\n",
    "    if (lines[i] != '\\n'):\n",
    "        item += lines[i].replace('\\n','')\n",
    "    else:\n",
    "        peotry.append(item)\n",
    "        item = ''\n",
    "\n",
    "peotry.remove('')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 预处理二：按照句号划分每条输入\n",
    "peotry_new = []\n",
    "for p in peotry:\n",
    "    li = p.split('。')\n",
    "    for s in li:\n",
    "        if (s != ''):\n",
    "            peotry_new.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 预处理三：按照字符分割每首诗词\n",
    "peotry_seg = []\n",
    "for p in peotry_new:\n",
    "    seg = []\n",
    "    for ch in p:\n",
    "        seg.append(ch)\n",
    "    peotry_seg.append(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 预处理四：定义词汇表\n",
    "\n",
    "# 将二维的tokens转成一维tokens\n",
    "all_characters = []\n",
    "for items in peotry_seg:\n",
    "    for item in items:\n",
    "        all_characters.append(item)\n",
    "\n",
    "# 生成原始的词汇集合\n",
    "unique_characters = []\n",
    "for token in all_characters:\n",
    "    if token not in unique_characters:\n",
    "        unique_characters.append(token)\n",
    "unique_characters.append('_PAD')\n",
    "unique_characters.append('_UNK')\n",
    "unique_characters.append('_STA')\n",
    "unique_characters.append('_END')\n",
    "\n",
    "# 统计每个词出现的频率\n",
    "dic = {}\n",
    "for token in unique_characters:\n",
    "    dic[token] = all_characters.count(token)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "## 预处理五：计算文本长度，统一添加padding\n",
    "\n",
    "# 计算每个句子的长度并存储\n",
    "lens = []\n",
    "for words in peotry_seg:\n",
    "    lens.append(len(words))\n",
    "print(max(lens))\n",
    "\n",
    "# 补全句子\n",
    "peotry_pad = []\n",
    "for words in peotry_seg:\n",
    "    words.append('_END')\n",
    "    words.insert(0,'_STA')\n",
    "    for i in range(max(lens)-len(words)+3):    ## 对于最大长度的句子，也再多补充一个_PAD\n",
    "        words.append('_PAD')\n",
    "    peotry_pad.append(words)\n",
    "\n",
    "\n",
    "lens = np.array(lens)+2\n",
    "\n",
    "##定义weights——weights：这个比较复杂，对于非PADDING输入1，PADDING输入0\n",
    "weights = []\n",
    "for item in lens:\n",
    "    weights.append(np.append(np.ones(item),np.zeros(max(lens)-item)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 预处理六：对每个词汇进行编码，并将每个数据进行表征\n",
    "vocab = {}\n",
    "for i in range(len(unique_characters)):\n",
    "    vocab[unique_characters[i]] = i\n",
    "\n",
    "representations = []\n",
    "for words in peotry_seg:\n",
    "    reps = []\n",
    "    for word in words:\n",
    "        if (word in unique_characters):\n",
    "            reps.append(vocab[word])\n",
    "        else:\n",
    "            reps.append(vocab['_UNK'])\n",
    "    representations.append(reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 预处理七：定义x，y\n",
    "x = []\n",
    "y = []\n",
    "for reps in representations:\n",
    "    x.append(reps[:(len(reps)-1)])\n",
    "    y.append(reps[1:len(reps)])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 预处理八：划分数据\n",
    "def data_division(x,y,lens):\n",
    "    n_train = int(len(x)*0.8)\n",
    "    n_test = int(len(x)*0.1)\n",
    "    n_dev = int(len(x) - n_train -n_test)\n",
    "\n",
    "    index_train = random.sample(range(len(x)),n_train)\n",
    "    index_dev = random.sample(list(set(range(len(x)))^set(index_train)) ,n_dev)\n",
    "    index_test = list(set(list(set(range(len(x)))^set(index_train)))^set(index_dev)) \n",
    "\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    weights_train = []\n",
    "    for index in index_train:\n",
    "        x_train.append(x[index])\n",
    "        y_train.append(y[index])\n",
    "        weights_train.append(weights[index])\n",
    "\n",
    "    x_dev = []\n",
    "    y_dev = []\n",
    "    weights_dev = []\n",
    "    for index in index_dev:\n",
    "        x_dev.append(x[index])\n",
    "        y_dev.append(y[index])\n",
    "        weights_dev.append(weights[index])\n",
    "\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    weights_test = []\n",
    "    for index in index_test:\n",
    "        x_test.append(x[index])\n",
    "        y_test.append(y[index])\n",
    "        weights_test.append(weights[index])\n",
    "\n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    x_dev = np.array(x_dev)\n",
    "    y_dev = np.array(y_dev)\n",
    "\n",
    "    x_test = np.array(x_test)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    return x_train,y_train,weights_train,x_dev,y_dev,weights_dev,x_test,y_test,weights_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 数据类\n",
    "class data_set(object):\n",
    "    def __init__(self):\n",
    "        data = data_division(x,y,lens)\n",
    "        self.x_train = data[0]\n",
    "        self.y_train = data[1]\n",
    "        self.weights_train = data[2]\n",
    "        self.x_dev = data[3]\n",
    "        self.y_dev = data[4]\n",
    "        self.weights_dev = data[5]\n",
    "        self.x_test = data[6]\n",
    "        self.y_test = data[7]\n",
    "        self.weights_test = data[8]            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Step2：RNN语言模型           参考：https://blog.csdn.net/felaim/article/details/70184697    https://www.cnblogs.com/wuzhitj/p/6297992.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 首先定义设置类内容\n",
    "class config(object):\n",
    "    init_scale = 0.1                            # 相关参数的初始值为随机均匀分布，范围是[-init_scale,+init_scale]\n",
    "    learning_rate = 1.0                         # 学习速率,在文本循环次数超过max_epoch以后会逐渐降低\n",
    "    max_grad_norm = 2                           # 用于控制梯度膨胀，如果梯度向量的L2模超过max_grad_norm，则等比例缩小\n",
    "    num_layers = 2                              # lstm层数\n",
    "    num_steps = max(lens)                       # 单个数据中，序列的长度。\n",
    "    hidden_size = 128                           # 隐藏层中单元数目\n",
    "    max_epoch = 2                               # epoch<max_epoch时，lr_decay值=1,epoch>max_epoch时,lr_decay逐渐减小\n",
    "    max_max_epoch = 5                           # 指的是整个文本循环次数。\n",
    "    keep_prob = 0.9                             # 用于dropout.每批数据输入时神经网络中的每个单元会以1-keep_prob的概率不工作，可以防止过拟合\n",
    "    lr_decay = 0.5                              # 学习速率衰减\n",
    "    batch_size = 50                             # 每批数据的规模，每批有20个。\n",
    "    vocab_size = len(unique_characters)         # 词典规模\n",
    "\n",
    "class val_config(object):\n",
    "    init_scale = 0.1                            # 相关参数的初始值为随机均匀分布，范围是[-init_scale,+init_scale]\n",
    "    learning_rate = 1.0                         # 学习速率,在文本循环次数超过max_epoch以后会逐渐降低\n",
    "    max_grad_norm = 2                           # 用于控制梯度膨胀，如果梯度向量的L2模超过max_grad_norm，则等比例缩小\n",
    "    num_layers = 2                              # lstm层数\n",
    "    num_steps = max(lens)                       # 单个数据中，序列的长度。\n",
    "    hidden_size = 128                           # 隐藏层中单元数目\n",
    "    max_epoch = 2                               # epoch<max_epoch时，lr_decay值=1,epoch>max_epoch时,lr_decay逐渐减小\n",
    "    max_max_epoch = 5                           # 指的是整个文本循环次数。\n",
    "    keep_prob = 0.9                             # 用于dropout.每批数据输入时神经网络中的每个单元会以1-keep_prob的概率不工作，可以防止过拟合\n",
    "    lr_decay = 0.5                              # 学习速率衰减\n",
    "    batch_size = 6                              # 每批数据的规模，每批有20个。\n",
    "    vocab_size = len(unique_characters)         # 词典规模\n",
    "    \n",
    "class eval_config(object):\n",
    "    init_scale = 0.1                            # 相关参数的初始值为随机均匀分布，范围是[-init_scale,+init_scale]\n",
    "    learning_rate = 1.0                         # 学习速率,在文本循环次数超过max_epoch以后会逐渐降低\n",
    "    max_grad_norm = 2                           # 用于控制梯度膨胀，如果梯度向量的L2模超过max_grad_norm，则等比例缩小\n",
    "    num_layers = 2                              # lstm层数\n",
    "    num_steps = max(lens)                       # 单个数据中，序列的长度。\n",
    "    hidden_size = 128                           # 隐藏层中单元数目\n",
    "    max_epoch = 2                               # epoch<max_epoch时，lr_decay值=1,epoch>max_epoch时,lr_decay逐渐减小\n",
    "    max_max_epoch = 5                           # 指的是整个文本循环次数。\n",
    "    keep_prob = 0.9                             # 用于dropout.每批数据输入时神经网络中的每个单元会以1-keep_prob的概率不工作，可以防止过拟合\n",
    "    lr_decay = 0.5                              # 学习速率衰减\n",
    "    batch_size = 1                              # 每批数据的规模，每批有20个。\n",
    "    vocab_size = len(unique_characters)         # 词典规模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 定义输入类内容\n",
    "class DataInput(object):\n",
    "    def __init__(self, config, data_set, division,name = None):\n",
    "        self.batch_size = batch_size = config.batch_size\n",
    "        self.num_steps = num_steps = config.num_steps\n",
    "        if (division == 'train'):\n",
    "            self.epoch_size = len(data_set.x_train) // batch_size  ##若输入数据是PTB类型整段文本数据，需要再除以num_steps进行划分，但对于已划分好的数据就不用再除以num_steps了\n",
    "            self.input_data = data_set.x_train\n",
    "            self.target_data = data_set.y_train\n",
    "            self.weights = data_set.weights_train\n",
    "        elif (division == 'dev'):\n",
    "            self.epoch_size = len(data_set.x_dev) // batch_size\n",
    "            self.input_data = data_set.x_dev\n",
    "            self.target_data = data_set.y_dev\n",
    "            self.weights = data_set.weights_dev\n",
    "        else:\n",
    "            self.epoch_size = len(data_set.x_test) // batch_size\n",
    "            self.input_data = data_set.x_test\n",
    "            self.target_data = data_set.y_test\n",
    "            self.weights = data_set.weights_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "案例中给出的epoch_size需要-1，这里不太明白为什么，本实验未-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## LSTM模型\n",
    "class LSTM_LM(object):\n",
    "    def __init__(self, is_training,config,input_):    ## config中定义模型需要得超参数，与输入无关；input_中定义输入方式以及相关得超参。\n",
    "        self._input = input_                          \n",
    "        batch_size = config.batch_size                ## 批大小在input_中，决定通过mini-batch方法每次输入得数据\n",
    "        num_steps = config.num_steps                  ## 步长对应得史RNN模型中每条输入得长度，这个也与输入有关\n",
    "        size = config.hidden_size                     ## 隐藏层大小属于设置里面的，决定的是embedding的维度\n",
    "        vocab_size = config.vocab_size                ## 词汇表大小决定的是embedding矩阵的大小\n",
    "        \n",
    "\n",
    "#定义input和target为placeholder\n",
    "        self._input_x = tf.placeholder(tf.int32, shape = [batch_size,num_steps])\n",
    "        self._target_y = tf.placeholder(tf.int32, shape = [batch_size,num_steps])\n",
    "        self._real_weights = tf.placeholder(tf.float32, shape = [batch_size,num_steps])\n",
    "        \n",
    "#设置默认的LSTM单元\n",
    "        def lstm_cell():\n",
    "            return tf.contrib.rnn.BasicLSTMCell(size, forget_bias = 0.0, state_is_tuple = True)  ## 定义BasicLSTMCell， state_is_tuple参数表明是否拆分输出和记忆为tuple，if False，输出和记忆会被连接\n",
    "        attn_cell = lstm_cell  ## 这里并不是调用函数，而是将函数赋给一个变量，保证if没运行的时候attn_cell也有值\n",
    "\n",
    "        if is_training and config.keep_prob < 1:  ## 只有训练时调用dropout，因此用index来指针，keep_prob表示保留的概率 \n",
    "            def attn_cell():\n",
    "                return tf.contrib.rnn.DropoutWrapper(lstm_cell(), output_keep_prob = config.keep_prob) ## 注意DropoutWrapper函数，output_keep_prob的含义表示保留率\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([attn_cell() for _ in range(config.num_layers)], state_is_tuple = True) ## for句法的使用，返回列表， 替代表达：[attn_cell()] * config.num_layers\n",
    "        self._initial_state = cell.zero_state(batch_size, tf.float32) ## 状态初始化，使用的是全0初始化\n",
    "        \n",
    "#创建网络的词嵌入的部分\n",
    "        with tf.device(\"/cpu:0\"):  ## 指定模型运行的设备\n",
    "            embedding = tf.get_variable(\"embedding\", [vocab_size, size], dtype = tf.float32)\n",
    "            inputs = tf.nn.embedding_lookup(embedding, self._input_x) ## lookup\n",
    "        if is_training and config.keep_prob < 1:  ## 注意这里的结构，在embedding层也是用了dropout来进行正则化\n",
    "            inputs = tf.nn.dropout(inputs, config.keep_prob)\n",
    "\n",
    "#定义输出\n",
    "        outputs = []\n",
    "        state = self._initial_state\n",
    "        with tf.variable_scope(\"RNN\"):  ## 上下文管理器，在RNN中，所有需要训练的variables都已经被封装到BasicLSTMCell中了\n",
    "            for time_step in range(num_steps):  ## 该循环是在一层LSTM中对于每个cell进行迭代运算，获取当前位置的输出以及状态\n",
    "                if time_step > 0:  \n",
    "                    tf.get_variable_scope().reuse_variables()  ## 当time_step>0时，说明最初的所有变量已经被赋值了，再次赋值的时候需要调用reuse\n",
    "                (cell_output, state) = cell(inputs[:, time_step, :], state)  ## 三维tensor与inputs相对应\n",
    "                outputs.append(cell_output)\n",
    "        \n",
    "        \n",
    "        output = tf.reshape(tf.concat(outputs, 1), [-1, size])  ## 这一步是把输出变为二维变量[batch_size*num_steps，size]的形状      \n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [size, vocab_size], dtype = tf.float32)\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size], dtype = tf.float32)\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b  ## Wx+b,计算线性部分，还未计算softmax\n",
    "        \n",
    "        weights = tf.reshape(self._real_weights,[-1])\n",
    "        \n",
    "        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(self._target_y, [-1])],[weights])\n",
    "        ## tf.contrib.legacy_seq2seq.sequence_loss_by_example()函数进行了如下的计算操作：计算每个sequence在每个位置上的log困惑度\n",
    "        ## logits:直接输入每个词汇的logits；targets：直接输入每个词汇的targets\n",
    "        \n",
    "        \n",
    "        ## 计算每个句子的困惑度\n",
    "        log_perplexity = tf.reduce_mean(tf.reshape(loss,[int(np.shape(loss)[0])//num_steps,num_steps]),axis = 1)\n",
    "        \n",
    "        self._cost = cost = tf.reduce_sum(log_perplexity) / batch_size  ## 平均log困惑度作为cost\n",
    "        self._final_state = state  ## 返回计算后的最终状态\n",
    "\n",
    "        if not is_training:  ## 对于验证集和测试集来说，不需要进行后项过程，到这里就结束了\n",
    "            return\n",
    "        \n",
    "#定义学习率，优化器等\n",
    "        self._lr = tf.Variable(0.0, trainable = False)  ## 定义学习率为一个变量，但是是不需要训练的变量\n",
    "\n",
    "        tvars = tf.trainable_variables()  ## tvars通过tf.trainable_variables()获得所有待训练的变量，返回的是列表\n",
    "                \n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), config.max_grad_norm)  \n",
    "        ## tf.gradients计算梯度，计算cost对于所有tvars中元素的梯度，返回梯度列表\n",
    "        ## tf.clip_by_global_norm用于梯度修剪，防止梯度爆炸——若梯度超过限值max_grad_norm,则梯度等比缩小\n",
    "        ## grads就是修建后的梯度值\n",
    "        \n",
    "        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(self._lr)  ## 梯度下降法\n",
    "        \n",
    "        \n",
    "        self._train_op = optimizer.apply_gradients(zip(grads, tvars),global_step = tf.contrib.framework.get_or_create_global_step())\n",
    "        ## 这一步是应用计算出来的梯度到优化器，这一步+计算梯度≈minimize\n",
    "        ## global_step学习率变化时常用的参数，用于计数，表征全局步数\n",
    "        ## tf.contrib.framework.get_or_create_global_step是用来返回或者创建globel_step变量的，初始从1开始；如果想从0开始，就直接定义一个变量tf.Variable(0, trainable=False) 就可以了\n",
    "\n",
    "        \n",
    "        ## 以下部分是学习率的更新\n",
    "        self._new_lr = tf.placeholder(tf.float32, shape = [], name = \"new_learning_rate\")  ## 这个placeholder是用来存放新新进来的学习率\n",
    "        self._lr_update = tf.assign(self._lr, self._new_lr)  \n",
    "        ## 用新的学习率替换原来的学习率,注意，此时的self._lr已经改变，同时self._lr_update被赋予的是一个操作，而不是具体的返回值\n",
    "        ## 因为tf.assign函数是会直接改变self._lr的操作，而不是返回值\n",
    "\n",
    "    def assign_lr(self, session, lr_value):  ## 参数是session\n",
    "        session.run(self._lr_update, feed_dict = {self._new_lr: lr_value})  ## 给新的学习率赋值\n",
    "\n",
    "#利用@property装饰器可以将返回变量设为只读\n",
    "    @property\n",
    "    def input(self):\n",
    "        return self._input\n",
    "    \n",
    "    @property\n",
    "    def input_x(self):\n",
    "        return self._input_x\n",
    "\n",
    "    @property\n",
    "    def target_y(self):\n",
    "        return self._target_y\n",
    "    \n",
    "    @property\n",
    "    def real_weights(self):\n",
    "        return self._real_weights\n",
    "    \n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "修改内容：\n",
    "（1）tf.contrib.legacy_seq2seq.sequence_loss_by_example()函数进行了如下的计算操作：计算每个sequence在每个位置上的log困惑度\n",
    "logits:直接输入每个词汇的logits；targets：直接输入每个词汇的targets\n",
    "（2）增加了困惑度的计算perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#定义训练一个epoch数据的函数\n",
    "def run_epoch(session, model, config, eval_op = None, verbose = False):\n",
    "    start_time = time.time() ## 记录运行时间\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    state = session.run(model.initial_state)  ## 初始化模型\n",
    "\n",
    "    fetches = {\n",
    "            \"cost\": model.cost,\n",
    "            \"final_state\": model.final_state,\n",
    "            }                              ## 要运行的操作\n",
    "\n",
    "    if eval_op is not None:\n",
    "        fetches[\"eval_op\"] = eval_op      ## 对于验证和测试不走反向过程\n",
    "        \n",
    "\n",
    "    for step in range(model.input.epoch_size):\n",
    "        feed_dict = {}  ## 需要定义喂入的数据\n",
    "        ## 每次输入一批数据\n",
    "        feed_dict[model.input_x] = model.input.input_data[step*config.batch_size:(step+1)*config.batch_size]\n",
    "        feed_dict[model.target_y] = model.input.target_data[step*config.batch_size:(step+1)*config.batch_size]\n",
    "        feed_dict[model.real_weights] = model.input.weights[step*config.batch_size:(step+1)*config.batch_size]\n",
    "        \n",
    "        for i, (c, h) in enumerate(model.initial_state):  ## 这里将c，h喂入到神经网络中\n",
    "            feed_dict[c] = state[i].c\n",
    "            feed_dict[h] = state[i].h\n",
    "        ## 将c，h作为喂入的数据，只能理解成在BasicLSTMCell中，c和h都是placeholder，具体还是不太明白，先保留。\n",
    "\n",
    "        vals = session.run(fetches, feed_dict)\n",
    "\n",
    "        cost = vals[\"cost\"]\n",
    "\n",
    "        state = vals[\"final_state\"]\n",
    "\n",
    "        costs += cost\n",
    "        iters += 1\n",
    "\n",
    "        if verbose:\n",
    "            print (\"%.3f perplexity: %.3f speed : %.0f wps\" \n",
    "                %(step * 1.0 / model.input.epoch_size, np.exp(costs / iters), \n",
    "                iters * model.input.batch_size / (time.time() - start_time)))\n",
    "    print(np.exp(costs / iters))\n",
    "    return np.exp(costs / iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对照源码，以下几个问题不太明白：\n",
    "（1）关于c，h作为占位符喂入模型的问题；\n",
    "（2）mini-batch方法中没有体现出分批喂入的代码,所以自行调整了代码显式加入了分批喂入；\n",
    "（3）这样对于训练、测试、验证分开训练，能不能共享参数——理论上似乎可以。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 初始化设置\n",
    "config = config()\n",
    "val_config = val_config()\n",
    "eval_config = eval_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting standard services.\n",
      "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "Epoch: 1 Learning rate: 1.000\n",
      "0.000 perplexity: 13.822 speed : 11 wps\n",
      "0.053 perplexity: 14.244 speed : 13 wps\n",
      "0.105 perplexity: 14.141 speed : 14 wps\n",
      "0.158 perplexity: 14.296 speed : 15 wps\n",
      "0.211 perplexity: 14.273 speed : 16 wps\n",
      "0.263 perplexity: 14.447 speed : 16 wps\n",
      "0.316 perplexity: 14.198 speed : 17 wps\n",
      "0.368 perplexity: 14.411 speed : 18 wps\n",
      "0.421 perplexity: 14.311 speed : 19 wps\n",
      "0.474 perplexity: 14.240 speed : 19 wps\n",
      "0.526 perplexity: 14.155 speed : 19 wps\n",
      "0.579 perplexity: 14.138 speed : 19 wps\n",
      "0.632 perplexity: 14.086 speed : 19 wps\n",
      "0.684 perplexity: 14.116 speed : 20 wps\n",
      "0.737 perplexity: 14.123 speed : 20 wps\n",
      "0.789 perplexity: 13.992 speed : 20 wps\n",
      "0.842 perplexity: 13.979 speed : 20 wps\n",
      "0.895 perplexity: 14.031 speed : 21 wps\n",
      "0.947 perplexity: 13.987 speed : 21 wps\n",
      "13.9867448618\n",
      "Epoch: 1 Train Perplexity: 13.987\n",
      "14.5804061544\n",
      "Epoch: 1 valid Perplexity: 14.580\n",
      "Epoch: 2 Learning rate: 1.000\n",
      "0.000 perplexity: 13.265 speed : 38 wps\n",
      "0.053 perplexity: 13.670 speed : 39 wps\n",
      "0.105 perplexity: 13.566 speed : 39 wps\n",
      "0.158 perplexity: 13.714 speed : 39 wps\n",
      "0.211 perplexity: 13.690 speed : 39 wps\n",
      "0.263 perplexity: 13.853 speed : 38 wps\n",
      "0.316 perplexity: 13.613 speed : 39 wps\n",
      "0.368 perplexity: 13.815 speed : 39 wps\n",
      "0.421 perplexity: 13.718 speed : 39 wps\n",
      "0.474 perplexity: 13.648 speed : 39 wps\n",
      "0.526 perplexity: 13.564 speed : 39 wps\n",
      "0.579 perplexity: 13.546 speed : 39 wps\n",
      "0.632 perplexity: 13.491 speed : 39 wps\n",
      "0.684 perplexity: 13.518 speed : 38 wps\n",
      "0.737 perplexity: 13.521 speed : 38 wps\n",
      "0.789 perplexity: 13.393 speed : 38 wps\n",
      "0.842 perplexity: 13.378 speed : 38 wps\n",
      "0.895 perplexity: 13.423 speed : 38 wps\n",
      "0.947 perplexity: 13.378 speed : 38 wps\n",
      "13.3776929653\n",
      "Epoch: 2 Train Perplexity: 13.378\n",
      "13.863602353\n",
      "Epoch: 2 valid Perplexity: 13.864\n",
      "Epoch: 3 Learning rate: 0.500\n",
      "0.000 perplexity: 12.614 speed : 39 wps\n",
      "0.053 perplexity: 12.999 speed : 38 wps\n",
      "0.105 perplexity: 12.901 speed : 37 wps\n",
      "0.158 perplexity: 13.046 speed : 37 wps\n",
      "0.211 perplexity: 13.027 speed : 37 wps\n",
      "0.263 perplexity: 13.183 speed : 38 wps\n",
      "0.316 perplexity: 12.960 speed : 38 wps\n",
      "0.368 perplexity: 13.156 speed : 39 wps\n",
      "0.421 perplexity: 13.068 speed : 39 wps\n",
      "0.474 perplexity: 13.004 speed : 39 wps\n",
      "0.526 perplexity: 12.927 speed : 39 wps\n",
      "0.579 perplexity: 12.913 speed : 39 wps\n",
      "0.632 perplexity: 12.864 speed : 40 wps\n",
      "0.684 perplexity: 12.892 speed : 40 wps\n",
      "0.737 perplexity: 12.898 speed : 40 wps\n",
      "0.789 perplexity: 12.780 speed : 40 wps\n",
      "0.842 perplexity: 12.768 speed : 40 wps\n",
      "0.895 perplexity: 12.814 speed : 40 wps\n",
      "0.947 perplexity: 12.774 speed : 40 wps\n",
      "12.773687443\n",
      "Epoch: 3 Train Perplexity: 12.774\n",
      "13.3037357843\n",
      "Epoch: 3 valid Perplexity: 13.304\n",
      "Epoch: 4 Learning rate: 0.250\n",
      "0.000 perplexity: 12.125 speed : 47 wps\n",
      "0.053 perplexity: 12.485 speed : 47 wps\n",
      "0.105 perplexity: 12.383 speed : 47 wps\n",
      "0.158 perplexity: 12.522 speed : 45 wps\n",
      "0.211 perplexity: 12.504 speed : 43 wps\n",
      "0.263 perplexity: 12.654 speed : 42 wps\n",
      "0.316 perplexity: 12.443 speed : 42 wps\n",
      "0.368 perplexity: 12.631 speed : 41 wps\n",
      "0.421 perplexity: 12.547 speed : 40 wps\n",
      "0.474 perplexity: 12.487 speed : 40 wps\n",
      "0.526 perplexity: 12.414 speed : 40 wps\n",
      "0.579 perplexity: 12.403 speed : 40 wps\n",
      "0.632 perplexity: 12.354 speed : 40 wps\n",
      "0.684 perplexity: 12.382 speed : 40 wps\n",
      "0.737 perplexity: 12.390 speed : 40 wps\n",
      "0.789 perplexity: 12.277 speed : 40 wps\n",
      "0.842 perplexity: 12.268 speed : 40 wps\n",
      "0.895 perplexity: 12.313 speed : 40 wps\n",
      "0.947 perplexity: 12.277 speed : 40 wps\n",
      "12.277061372\n",
      "Epoch: 4 Train Perplexity: 12.277\n",
      "12.8327475745\n",
      "Epoch: 4 valid Perplexity: 12.833\n",
      "Epoch: 5 Learning rate: 0.125\n",
      "0.000 perplexity: 11.725 speed : 37 wps\n",
      "0.053 perplexity: 12.062 speed : 39 wps\n",
      "0.105 perplexity: 11.956 speed : 38 wps\n",
      "0.158 perplexity: 12.090 speed : 39 wps\n",
      "0.211 perplexity: 12.071 speed : 39 wps\n",
      "0.263 perplexity: 12.215 speed : 39 wps\n",
      "0.316 perplexity: 12.013 speed : 39 wps\n",
      "0.368 perplexity: 12.196 speed : 39 wps\n",
      "0.421 perplexity: 12.117 speed : 39 wps\n",
      "0.474 perplexity: 12.061 speed : 39 wps\n",
      "0.526 perplexity: 11.991 speed : 39 wps\n",
      "0.579 perplexity: 11.983 speed : 39 wps\n",
      "0.632 perplexity: 11.938 speed : 39 wps\n",
      "0.684 perplexity: 11.967 speed : 39 wps\n",
      "0.737 perplexity: 11.977 speed : 39 wps\n",
      "0.789 perplexity: 11.871 speed : 39 wps\n",
      "0.842 perplexity: 11.865 speed : 39 wps\n",
      "0.895 perplexity: 11.911 speed : 39 wps\n",
      "0.947 perplexity: 11.879 speed : 39 wps\n",
      "11.8792480924\n",
      "Epoch: 5 Train Perplexity: 11.879\n",
      "12.4871981008\n",
      "Epoch: 5 valid Perplexity: 12.487\n",
      "11.794243407\n",
      "Test Perplexity: 11.794\n"
     ]
    }
   ],
   "source": [
    "## 创建图\n",
    "with tf.Graph().as_default():\n",
    "    initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n",
    "    data_input = data_set()\n",
    "\n",
    "    with tf.name_scope(\"Train\"):\n",
    "        train_input = DataInput(config,data_input,'train')\n",
    "        with tf.variable_scope(\"Model\", reuse = None, initializer = initializer):\n",
    "            m = LSTM_LM(is_training = True, config = config, input_ = train_input)\n",
    "\n",
    "    with tf.name_scope(\"Valid\"):\n",
    "        valid_input = DataInput(val_config,data_input,'dev')\n",
    "        with tf.variable_scope(\"Model\", reuse = True, initializer = initializer):\n",
    "            mvalid = LSTM_LM(is_training = False, config = val_config, input_ = valid_input)\n",
    "\n",
    "    with tf.name_scope(\"Test\"):\n",
    "        test_input = DataInput(eval_config,data_input,'test')\n",
    "        with tf.variable_scope(\"Model\", reuse = True, initializer = initializer):\n",
    "            mtest = LSTM_LM(is_training = False, config = eval_config, input_ = test_input)\n",
    "            \n",
    "## 创建训练的管理器\n",
    "        sv = tf.train.Supervisor()\n",
    "        with sv.managed_session() as session:\n",
    "            for i in range(config.max_max_epoch):    ## 需要循环训练的总次数\n",
    "                lr_decay = config.lr_decay ** max(i + 1 - config.max_epoch, 0.0)  ## 按照指数衰减，本实验梅西衰减为一半  \n",
    "                m.assign_lr(session, config.learning_rate * lr_decay)  ## 更换学习率\n",
    "\n",
    "                print(\"Epoch: %d Learning rate: %.3f\" %(i + 1, session.run(m.lr)))  \n",
    "                train_perplexity = run_epoch(session, m,config ,eval_op = m.train_op, verbose = True)  ## 训练\n",
    "                print(\"Epoch: %d Train Perplexity: %.3f\" %(i + 1, train_perplexity))  \n",
    "                valid_perplexity = run_epoch(session, mvalid, val_config)  ## 验证\n",
    "                print(\"Epoch: %d valid Perplexity: %.3f\" %(i + 1, valid_perplexity))\n",
    "\n",
    "            test_perplexity = run_epoch(session, mtest, eval_config)      ## 测试\n",
    "            print(\"Test Perplexity: %.3f\" %test_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
