{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CRFPP\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step1:获取文本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 读入CoNLL Corpus\n",
    "def read_CoNLL(filename):\n",
    "    docs = []\n",
    "    word = []\n",
    "    POS = []\n",
    "    chunking = []\n",
    "    NE = []\n",
    "    doc = []\n",
    "    \n",
    "    f = open(filename,encoding = 'gbk')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    i = 0\n",
    "    for line in lines:\n",
    "        line = line.replace('\\n','')\n",
    "        if line == '-DOCSTART- -X- O O':           \n",
    "            if i != 0:\n",
    "                docs.append(doc)\n",
    "                doc = []\n",
    "            doc.append('-DOCSTART-')\n",
    "        else:\n",
    "            if line != '':\n",
    "                doc.append(line.split(' ')[0])\n",
    "            else:\n",
    "                doc.append('_space')\n",
    "        \n",
    "        \n",
    "        if (line != ''):\n",
    "            labels = line.split(' ')\n",
    "            word.append(labels[0])\n",
    "            POS.append(labels[1])\n",
    "            chunking.append(labels[2])\n",
    "            NE.append(labels[3])\n",
    "        else:\n",
    "            word.append('_space')\n",
    "            POS.append('_space')\n",
    "            chunking.append('_space')\n",
    "            NE.append('_space')\n",
    "        \n",
    "        i += 1\n",
    "    docs.append(doc)\n",
    "\n",
    "    df = pd.DataFrame(data = np.transpose(np.array([word,POS,chunking,NE])), columns = ['word','POS','chunking','NE'])\n",
    "    return df, docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,docs_train = read_CoNLL('eng.train')\n",
    "df_test,docs_test = read_CoNLL('eng.testa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train = docs_train[:30]\n",
    "docs_test = docs_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 0\n",
    "for i in range(len(docs_train)):\n",
    "    for j in range(len(docs_train[i])):\n",
    "        n_train += 1\n",
    "\n",
    "n_test = 0\n",
    "for i in range(len(docs_test)):\n",
    "    for j in range(len(docs_test[i])):\n",
    "        n_test += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[:n_train]\n",
    "df_test = df_test[:n_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 常用词列表（至少在五个文档中出现过的词汇）\n",
    "def create_vocab(data):\n",
    "    vocab = {}\n",
    "    if type(data) == pd.core.frame.DataFrame:       \n",
    "        keys = set(data['word'])\n",
    "        for key in keys:\n",
    "            vocab[key] = data['word'].tolist().count(key)\n",
    "    else:\n",
    "        keys = []\n",
    "        for d in data:\n",
    "            if d not in keys:\n",
    "                keys.append(d)\n",
    "        for key in keys:\n",
    "            if type(key) == list:\n",
    "                vocab[[(x,y) for x,y in [key]][0]] = data.count(key)\n",
    "            else:\n",
    "                vocab[key] = data.count(key)\n",
    "    return vocab\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = create_vocab(df_train)\n",
    "\n",
    "common_vocab = []\n",
    "for word in list(vocab.keys()):\n",
    "    n = 0\n",
    "    for doc in docs_train:\n",
    "        if word in doc:\n",
    "            n += 1\n",
    "    \n",
    "    if n >= 5:\n",
    "        common_vocab.append(word)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step2:抽取基本特征相关的词列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 一元相关性模型：（N+ - N-）/(N+ * N-)^0.5\n",
    "def unigram_correlation(df,label):\n",
    "    word_list = [df.loc[i-1][0] for i in range(1,len(df)) if df.loc[i][3] == label]\n",
    "    correlation_list = [[key,(val*2-len(word_list))/(val*(len(word_list)-val)+1)**0.5] for key,val in zip(list(create_vocab(word_list).keys()),list(create_vocab(word_list).values()))]\n",
    "    return sorted(correlation_list,key=operator.itemgetter(1),reverse=True)[:20]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_co_train = {}\n",
    "for label in set(df_train['NE']):\n",
    "    if (label != 'O' and label != '_space'):\n",
    "        uni_co_train[label] = unigram_correlation(df_train,label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['_space', -1.5343265552504246],\n",
       " ['in', -2.931521304691667],\n",
       " ['the', -3.1032797684331617],\n",
       " ['to', -4.09428268714651],\n",
       " ['(', -4.09428268714651],\n",
       " ['and', -4.962694350081032],\n",
       " ['with', -4.962694350081032],\n",
       " ['from', -5.262019756089627],\n",
       " ['West', -5.613835722137896],\n",
       " ['of', -6.557109247067381],\n",
       " ['/', -7.223568439694461],\n",
       " ['-', -7.223568439694461],\n",
       " ['that', -8.121212121212121],\n",
       " ['on', -8.121212121212121],\n",
       " ['northern', -9.428808992889305],\n",
       " ['between', -9.428808992889305],\n",
       " ['Abu', -9.428808992889305],\n",
       " ['by', -11.608677113915903],\n",
       " ['says', -11.608677113915903],\n",
       " ['accused', -11.608677113915903]]"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_co_train['I-LOC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 二元相关性模型：\n",
    "def bigram_correlation(df,label):\n",
    "    word_list = [[df.loc[i-2][0],df.loc[i-1][0]] for i in range(2,len(df)) if df.loc[i][3] == label]\n",
    "    correlation_list = [[key,(val*2-len(word_list))/(val*(len(word_list)-val)+1)**0.5] for key,val in zip(list(create_vocab(word_list).keys()),list(create_vocab(word_list).values()))]\n",
    "    return sorted(correlation_list,key=operator.itemgetter(1),reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_co_train = {}\n",
    "for label in set(df_train['NE']):\n",
    "    if (label != 'O' and label != '_space'):\n",
    "        bi_co_train[label] = bigram_correlation(df_train,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('.', '_space'), -2.242843314993112],\n",
       " [('the', 'West'), -6.036161376351597],\n",
       " [('1996-08-22', '_space'), -6.557109247067381],\n",
       " [('to', 'the'), -6.557109247067381],\n",
       " [('-DOCSTART-', '_space'), -7.223568439694461],\n",
       " [('in', 'the'), -7.223568439694461],\n",
       " [('talks', 'with'), -9.428808992889305],\n",
       " [('\"', '_space'), -9.428808992889305],\n",
       " [('DIGEST', '-'), -9.428808992889305],\n",
       " [('reports', 'from'), -11.608677113915903],\n",
       " [('Britain', 'and'), -11.608677113915903],\n",
       " [('sheep', 'from'), -11.608677113915903],\n",
       " [('visit', 'to'), -11.608677113915903],\n",
       " [('negotiator', 'with'), -11.608677113915903],\n",
       " [('SALE', 'LIMITS'), -11.608677113915903],\n",
       " [('US', '/'), -11.608677113915903],\n",
       " [('UK', '/'), -11.608677113915903],\n",
       " [('the', 'United'), -11.608677113915903],\n",
       " [('support', 'to'), -11.608677113915903],\n",
       " [('in', 'northern'), -11.608677113915903]]"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_co_train['I-LOC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 词后缀列表：\n",
    "def NE_suffix(df,label):\n",
    "    suffix_list = [df.loc[i][0][-3:-1]+df.loc[i][0][-1] for i in range(0,len(df)) if df.loc[i][3] == label]\n",
    "    correlation_list = [[key,(val*2-len(suffix_list))/(val*(len(suffix_list)-val)+1)**0.5] for key,val in zip(list(create_vocab(suffix_list).keys()),list(create_vocab(suffix_list).values()))]\n",
    "    return sorted(correlation_list,key=operator.itemgetter(1),reverse=True)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_train = {}\n",
    "for label in set(df_train['NE']):\n",
    "    if (label != 'O' and label != '_space'):\n",
    "        suffix_train[label] = NE_suffix(df_train,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['raq', -3.7826047922906163],\n",
       " ['ael', -3.9310970484186476],\n",
       " ['ria', -4.962694350081032],\n",
       " ['ain', -5.262019756089627],\n",
       " ['wan', -5.613835722137896],\n",
       " ['dad', -5.613835722137896],\n",
       " ['est', -5.613835722137896],\n",
       " ['ank', -5.613835722137896],\n",
       " ['any', -6.036161376351597],\n",
       " ['ina', -6.557109247067381],\n",
       " ['tan', -6.557109247067381],\n",
       " ['cus', -7.223568439694461],\n",
       " ['bya', -7.223568439694461],\n",
       " ['ran', -7.223568439694461],\n",
       " ['aza', -7.223568439694461],\n",
       " ['UAE', -7.223568439694461],\n",
       " ['nce', -8.121212121212121],\n",
       " ['DON', -8.121212121212121],\n",
       " ['cow', -8.121212121212121],\n",
       " ['har', -8.121212121212121],\n",
       " ['non', -8.121212121212121],\n",
       " ['pan', -8.121212121212121],\n",
       " ['pei', -9.428808992889305],\n",
       " ['ait', -9.428808992889305],\n",
       " ['LEM', -9.428808992889305],\n",
       " ['ton', -9.428808992889305],\n",
       " ['lan', -9.428808992889305],\n",
       " ['and', -9.428808992889305],\n",
       " ['DAD', -9.428808992889305],\n",
       " ['key', -9.428808992889305],\n",
       " ['lah', -9.428808992889305],\n",
       " ['Abu', -9.428808992889305],\n",
       " ['abi', -9.428808992889305],\n",
       " ['sia', -9.428808992889305],\n",
       " ['run', -9.428808992889305],\n",
       " ['lia', -9.428808992889305],\n",
       " ['onn', -11.608677113915903],\n",
       " ['.S.', -11.608677113915903],\n",
       " ['ING', -11.608677113915903],\n",
       " ['ine', -11.608677113915903],\n",
       " ['ing', -11.608677113915903],\n",
       " ['ENS', -11.608677113915903],\n",
       " ['US', -11.608677113915903],\n",
       " ['UK', -11.608677113915903],\n",
       " ['ted', -11.608677113915903],\n",
       " ['tes', -11.608677113915903],\n",
       " ['bul', -11.608677113915903],\n",
       " ['rut', -11.608677113915903],\n",
       " ['uth', -11.608677113915903],\n",
       " ['ica', -11.608677113915903]]"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suffix_train['I-LOC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 命名实体后缀列表：\n",
    "def NE_word_suffix(df,label):\n",
    "    NE_suffix_list = []\n",
    "    new_label = label.split('-')\n",
    "    for i in range(len(df)):        \n",
    "        if (df.loc[i][3] != 'O' and df.loc[i][3] != '_space'):\n",
    "            if (i != len(df)-1):\n",
    "                if (df.loc[i][3].split('-')[1] == new_label[1] and df.loc[i+1][3] != label):\n",
    "                    NE_suffix_list.append(df.loc[i][0])\n",
    "            else:\n",
    "                if (df.loc[i][3].split('-')[1] == new_label[1]):\n",
    "                    NE_suffix_list.append(df.loc[i][0])\n",
    "    \n",
    "    correlation_list = [[key,(val*2-len(NE_suffix_list))/(val*(len(NE_suffix_list)-val)+1)**0.5] for key,val in zip(list(create_vocab(NE_suffix_list).keys()),list(create_vocab(NE_suffix_list).values()))]\n",
    "    return sorted(correlation_list,key=operator.itemgetter(1),reverse=True)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "NE_word_suffix_train = {}\n",
    "for label in set(df_train['NE']):\n",
    "    if (label != 'O' and label != '_space' and label != 'B-LOC' and label != 'B-ORG' and label != 'B-PER' and label != 'B-MISC'):\n",
    "        NE_word_suffix_train[label] = NE_word_suffix(df_train,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Arafat', -3.202647210128267],\n",
       " ['Peres', -4.212676001067371],\n",
       " ['Fischler', -4.602654792369785],\n",
       " ['Zhirinovsky', -4.602654792369785],\n",
       " ['Hendrix', -5.0988796285533615],\n",
       " ['Netanyahu', -5.0988796285533615],\n",
       " ['Simitis', -5.763633318189896],\n",
       " ['Skandalidis', -5.763633318189896],\n",
       " ['Levy', -6.726681467091999],\n",
       " ['Saddam', -6.726681467091999],\n",
       " ['Hussein', -6.726681467091999],\n",
       " ['Zwingmann', -8.322966839023469],\n",
       " ['Jones', -8.322966839023469],\n",
       " ['Shubei', -8.322966839023469],\n",
       " ['Tang', -8.322966839023469],\n",
       " ['Kontogiannis', -8.322966839023469],\n",
       " ['Rabinovich', -8.322966839023469],\n",
       " ['Awdankiewicz', -8.322966839023469],\n",
       " ['Rdainah', -8.322966839023469],\n",
       " ['Rabbani', -8.322966839023469],\n",
       " ['Hariri', -8.322966839023469],\n",
       " ['Blackburn', -11.875503619084816],\n",
       " ['Pas', -11.875503619084816],\n",
       " ['Palacio', -11.875503619084816],\n",
       " ['Etchingham', -11.875503619084816],\n",
       " ['Chan', -11.875503619084816],\n",
       " ['Guofang', -11.875503619084816],\n",
       " ['Lien', -11.875503619084816],\n",
       " ['Dominion', -11.875503619084816],\n",
       " ['Siegel', -11.875503619084816],\n",
       " ['Ben-Elissar', -11.875503619084816],\n",
       " ['al-', -11.875503619084816],\n",
       " ['Assad', -11.875503619084816],\n",
       " ['God', -11.875503619084816],\n",
       " ['Hafidh', -11.875503619084816],\n",
       " ['Rajavi', -11.875503619084816],\n",
       " ['Rastegar', -11.875503619084816],\n",
       " ['Gush', -11.875503619084816],\n",
       " ['Berri', -11.875503619084816],\n",
       " ['Sfeir', -11.875503619084816],\n",
       " ['Maslowe', -11.875503619084816],\n",
       " ['Bath', -11.875503619084816],\n",
       " ['Auchard', -11.875503619084816],\n",
       " ['Juppe', -11.875503619084816],\n",
       " ['Goydos', -11.875503619084816],\n",
       " ['Mayfair', -11.875503619084816],\n",
       " ['Tanaka', -11.875503619084816],\n",
       " ['Stricker', -11.875503619084816],\n",
       " ['Leonard', -11.875503619084816],\n",
       " ['Brooks', -11.875503619084816]]"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NE_word_suffix_train['I-PER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 功能词汇（命名实体中出现的小写词汇）\n",
    "def function_word(df,label):\n",
    "    function_word_list = [df.loc[i][0] for i in range(0,len(df)) if (df.loc[i][3] == label and df.loc[i][0].islower() == True)]\n",
    "    correlation_list = [[key,(val*2-len(function_word_list))/(val*(len(function_word_list)-val)+1)**0.5] for key,val in zip(list(create_vocab(function_word_list).keys()),list(create_vocab(function_word_list).values()))]\n",
    "    return sorted(correlation_list,key=operator.itemgetter(1),reverse=True)[:20]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_word_train = {}\n",
    "for label in set(df_train['NE']):\n",
    "    if (label != 'O' and label != '_space' and label != 'B-LOC' and label != 'B-ORG' and label != 'B-PER' and label != 'B-MISC'):\n",
    "        function_word_train[label] = function_word(df_train,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step3：局部特征选取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "## POS tag\n",
    "tag2index = {}\n",
    "index2tag = {}\n",
    "n = 2\n",
    "tag2index['punc'] = 1\n",
    "index2tag[1] = 'punc'\n",
    "tag2index['unk'] = 0\n",
    "index2tag[0] = 'unk'\n",
    "for item in set(df_train['POS']):\n",
    "    if (item.isupper() == True):\n",
    "        tag2index[item] = n\n",
    "        index2tag[n] = item\n",
    "        n += 1\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_tagger(df):\n",
    "    tag_list =[]\n",
    "    for tag in df['POS']:\n",
    "        if tag in list(tag2index.keys()):\n",
    "            tag_list.append(tag2index[tag])\n",
    "        else:\n",
    "            if tag.isupper() == False:\n",
    "                tag_list.append(tag2index['punc'])\n",
    "            else:\n",
    "                tag_list.append(tag2index['unk'])\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_tag_train = POS_tagger(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_tag_test = POS_tagger(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 句首词，大小写，文档的位置（四个参数：首字母大写与否、是不是句首词、全大写与否、文档的位置（头文件:1，标题:2，信息:3，文本:4））\n",
    "def first_capital(df):\n",
    "    tag_list = []\n",
    "    for word in df['word']:\n",
    "        if word[0].isupper():\n",
    "            tag_list.append(1)\n",
    "        else:\n",
    "            tag_list.append(0)\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_word(df):\n",
    "    tag_list = []\n",
    "    for i in range(len(df)):\n",
    "        if (i>0):\n",
    "            if (df['word'][i-1] == '_space' or df['word'][i-1] == '.' or df['word'][i-1] == '!' or df['word'][i-1] == '?'):\n",
    "                tag_list.append(1)\n",
    "            else:\n",
    "                tag_list.append(0)\n",
    "        else:\n",
    "            tag_list.append(1)\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_capitals(df):\n",
    "    tag_list = []\n",
    "    for word in df['word']:\n",
    "        if word.isupper():\n",
    "            tag_list.append(1)\n",
    "        else:\n",
    "            tag_list.append(0)\n",
    "    return tag_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_locations(df,docs):\n",
    "    tag_list = []\n",
    "    docs_lens = []\n",
    "    docs_segments = []\n",
    "    for doc in docs:\n",
    "        split_points = []\n",
    "        i = 0\n",
    "        while (i < 3):\n",
    "            for n in range(len(doc)):\n",
    "                if doc[n] == '_space':\n",
    "                    split_points.append(n)\n",
    "                    i += 1\n",
    "        docs_segments.append(split_points)\n",
    "        docs_lens.append(len(doc))\n",
    "        \n",
    "    \n",
    "    num_words = 0\n",
    "    for j in range(len(docs_lens)):\n",
    "        check_words = df['word'][num_words:num_words+docs_lens[j]]\n",
    "        for cw in range(len(check_words)):\n",
    "            if (cw <= docs_segments[j][0]):\n",
    "                tag_list.append(1)\n",
    "            elif (cw > docs_segments[j][0] and cw <= docs_segments[j][1]):\n",
    "                tag_list.append(2)\n",
    "            elif (cw > docs_segments[j][1] and cw <= docs_segments[j][2]):\n",
    "                tag_list.append(3)\n",
    "            else:\n",
    "                tag_list.append(4)\n",
    "        num_words += docs_lens[j]\n",
    "    \n",
    "    if (len(tag_list) == len(df)):\n",
    "        return tag_list\n",
    "    else:\n",
    "        print('error')\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_cap_train = first_capital(df_train)\n",
    "first_word_train = first_word(df_train)\n",
    "all_capitals_train = all_capitals(df_train)\n",
    "doc_loc_train = document_locations(df_train,docs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_cap_test = first_capital(df_test)\n",
    "first_word_test = first_word(df_test)\n",
    "all_capitals_test = all_capitals(df_test)\n",
    "doc_loc_test = document_locations(df_test,docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 语言符号信息 （主要包括. , / % $ -） 以及数字信息\n",
    "def symbal_information(df):\n",
    "    tag_list = []\n",
    "    for word in df['word']:\n",
    "        if ('.' in word or ',' in word or '/' in word or '%' in word or '$' in word or '-' in word):\n",
    "            tag_list.append(1)\n",
    "        else:\n",
    "            tag_list.append(0)\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_information(df):\n",
    "    tag_list = []\n",
    "    for word in df['word']:\n",
    "        if (re.search('\\d',word) != None):\n",
    "            tag_list.append(1)\n",
    "        else:\n",
    "            tag_list.append(0)\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbal_infor_train = symbal_information(df_train)\n",
    "number_infor_train = number_information(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbal_infor_test = symbal_information(df_test)\n",
    "number_infor_test = number_information(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 引号，括号信息\n",
    "def filled_information(df):\n",
    "    li_1 = []\n",
    "    li_2 = []\n",
    "    li_3 = []\n",
    "    li_4 = []\n",
    "    li_5 = []\n",
    "    n_1 = 0\n",
    "    n_2 = 0\n",
    "    n_3 = 0\n",
    "    n_4 = 0\n",
    "    n_5 = 0\n",
    "    for word in df['word']:\n",
    "        if (word == '('):\n",
    "            n_1 = 1\n",
    "        if (word == ')'):\n",
    "            n_1 = 0\n",
    "        if (word == '['):\n",
    "            n_2 = 1\n",
    "        if (word == ']'):\n",
    "            n_2 = 0\n",
    "        if (word == '{'):\n",
    "            n_3 = 1\n",
    "        if (word == '}'):\n",
    "            n_3 =0\n",
    "        if (word == \"'\"):\n",
    "            n_4 += 1\n",
    "        if (word == '\"'):\n",
    "            n_5 += 1\n",
    "        li_1.append(n_1)\n",
    "        li_2.append(n_2)\n",
    "        li_3.append(n_3)\n",
    "        li_4.append(n_4%2)\n",
    "        li_5.append(n_5%2)\n",
    "        \n",
    "    tag_list = [int(x) for x in list(np.array(li_1)+np.array(li_2)+np.array(li_3)+np.array(li_4)+np.array(li_5))]\n",
    "    \n",
    "    return tag_list\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil_infor_train = filled_information(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil_infor_test = filled_information(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 常用词列表\n",
    "def common_word(df):\n",
    "    tag_list = []\n",
    "    for word in df['word']:\n",
    "        if word not in common_vocab:\n",
    "            tag_list.append(1)\n",
    "        else:\n",
    "            tag_list.append(0)\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_word_train =  common_word(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_word_test =  common_word(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 二元特征提取\n",
    "def bi_feature(df,label):\n",
    "    tag_list = []\n",
    "    tag_list.append(0)\n",
    "    tag_list.append(0)\n",
    "    for i in range(2,len(df)):\n",
    "        if (df['NE'][i] != 'O' and df['NE'][i] != '_space'):\n",
    "            if (df['NE'][i].split('-')[1] == label and (df['word'][i-2],df['word'][i-1]) in [x for [x,y] in bi_co_train['I-'+label]]):\n",
    "                tag_list.append(1)\n",
    "            else:\n",
    "                tag_list.append(0)\n",
    "        else:\n",
    "            tag_list.append(0)\n",
    "    return tag_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_LOC_train = bi_feature(df_train,'LOC')\n",
    "bi_PER_train = bi_feature(df_train,'PER')\n",
    "bi_ORG_train = bi_feature(df_train,'ORG')\n",
    "bi_MISC_train = bi_feature(df_train,'MISC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_LOC_test = bi_feature(df_test,'LOC')\n",
    "bi_PER_test = bi_feature(df_test,'PER')\n",
    "bi_ORG_test = bi_feature(df_test,'ORG')\n",
    "bi_MISC_test = bi_feature(df_test,'MISC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 词后缀特征\n",
    "def suffix_feature(df,label):\n",
    "    tag_list = []\n",
    "    for i in range(len(df)):\n",
    "        if (df['NE'][i] != 'O' and df['NE'][i] != '_space'):\n",
    "            if (df['NE'][i].split('-')[1] == label and df['word'][i][-3:-1]+df['word'][i][-1] in [x for [x,y] in suffix_train['I-'+label]]):\n",
    "                tag_list.append(1)\n",
    "            else:\n",
    "                tag_list.append(0)\n",
    "        else:\n",
    "            tag_list.append(0)\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_LOC_train = suffix_feature(df_train,'LOC')\n",
    "suffix_PER_train = suffix_feature(df_train,'PER')\n",
    "suffix_ORG_train = suffix_feature(df_train,'ORG')\n",
    "suffix_MISC_train = suffix_feature(df_train,'MISC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_LOC_test = suffix_feature(df_test,'LOC')\n",
    "suffix_PER_test = suffix_feature(df_test,'PER')\n",
    "suffix_ORG_test = suffix_feature(df_test,'ORG')\n",
    "suffix_MISC_test = suffix_feature(df_test,'MISC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 类别后缀特征(一元特征)\n",
    "def c_suffix_feature(df,label):\n",
    "    tag_list = []\n",
    "    for i in range(len(df)-1):\n",
    "        if (df['NE'][i+1] != 'O' and df['NE'][i+1] != '_space'):\n",
    "            if (df['NE'][i+1].split('-')[1] == label and df['word'][i+1][0].isupper() == True and df['word'][i][0].isupper() == True):\n",
    "                tag_list.append(1)\n",
    "            else:\n",
    "                tag_list.append(0)\n",
    "        else:\n",
    "            tag_list.append(0)\n",
    "    tag_list.append(0)\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "csuffix_LOC_train = c_suffix_feature(df_train,'LOC')\n",
    "csuffix_PER_train = c_suffix_feature(df_train,'PER')\n",
    "csuffix_ORG_train = c_suffix_feature(df_train,'ORG')\n",
    "csuffix_MISC_train = c_suffix_feature(df_train,'MISC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "csuffix_LOC_test = c_suffix_feature(df_test,'LOC')\n",
    "csuffix_PER_test = c_suffix_feature(df_test,'PER')\n",
    "csuffix_ORG_test = c_suffix_feature(df_test,'ORG')\n",
    "csuffix_MISC_test = c_suffix_feature(df_test,'MISC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 功能词汇特征\n",
    "def functional_word_train(df,label):\n",
    "    tag_list = []\n",
    "    for word in df['word']:\n",
    "        if word in [x for [x,y] in function_word_train[label]]:\n",
    "            tag_list.append(1)\n",
    "        else:\n",
    "            tag_list.append(0)\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "functional_LOC_train = functional_word_train(df_train,'I-LOC')\n",
    "functional_PER_train = functional_word_train(df_train,'I-PER')\n",
    "functional_ORG_train = functional_word_train(df_train,'I-ORG')\n",
    "functional_MISC_train = functional_word_train(df_train,'I-MISC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "functional_LOC_test = functional_word_train(df_test,'I-LOC')\n",
    "functional_PER_test = functional_word_train(df_test,'I-PER')\n",
    "functional_ORG_test = functional_word_train(df_test,'I-ORG')\n",
    "functional_MISC_test = functional_word_train(df_test,'I-MISC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step4：全局特征选取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 全局一元特征 \n",
    "def global_uni_feature(docs,label):\n",
    "    tag_list = []\n",
    "    n = 0\n",
    "    for i in range(len(docs)):\n",
    "        for j in range(len(docs[i])):\n",
    "            inds = [a-1 for a,b in enumerate(docs[i]) if b==docs[i][j]]\n",
    "            inds.remove(j-1)\n",
    "            is_uni = 0\n",
    "            for ind in inds:\n",
    "                if docs[i][ind] in [x for [x,_] in uni_co_train[label]]:\n",
    "                    is_uni = 1\n",
    "            tag_list.append(is_uni)     \n",
    "            n += 1\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "guni_LOC_train = global_uni_feature(docs_train,'I-LOC')\n",
    "guni_PER_train = global_uni_feature(docs_train,'I-PER')\n",
    "guni_ORG_train = global_uni_feature(docs_train,'I-ORG')\n",
    "guni_MISC_train = global_uni_feature(docs_train,'I-MISC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "guni_LOC_test = global_uni_feature(docs_test,'I-LOC')\n",
    "guni_PER_test = global_uni_feature(docs_test,'I-PER')\n",
    "guni_ORG_test = global_uni_feature(docs_test,'I-ORG')\n",
    "guni_MISC_test = global_uni_feature(docs_test,'I-MISC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 全局二元特征 \n",
    "def global_bi_feature(docs,label):\n",
    "    tag_list = []\n",
    "    n = 0\n",
    "    for i in range(len(docs)):\n",
    "        for j in range(len(docs[i])):\n",
    "            inds = [(a-2,a-1) for a,b in enumerate(docs[i]) if b==docs[i][j]]\n",
    "            inds.remove((j-2,j-1))\n",
    "            is_bi = 0\n",
    "            for ind in inds:\n",
    "                if (docs[i][ind[0]],docs[i][ind[1]]) in [x for [x,_] in bi_co_train[label]]:\n",
    "                    is_bi = 1\n",
    "            tag_list.append(is_bi)     \n",
    "            n += 1\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbi_LOC_train = global_bi_feature(docs_train,'I-LOC')\n",
    "gbi_PER_train = global_bi_feature(docs_train,'I-PER')\n",
    "gbi_ORG_train = global_bi_feature(docs_train,'I-ORG')\n",
    "gbi_MISC_train = global_bi_feature(docs_train,'I-MISC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbi_LOC_test = global_bi_feature(docs_test,'I-LOC')\n",
    "gbi_PER_test = global_bi_feature(docs_test,'I-PER')\n",
    "gbi_ORG_test = global_bi_feature(docs_test,'I-ORG')\n",
    "gbi_MISC_test = global_bi_feature(docs_test,'I-MISC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 全局词后缀特征 \n",
    "def global_csuffix_feature(docs,label,csuffix):\n",
    "    tag_list = []\n",
    "    n = 0\n",
    "    for i in range(len(docs)):\n",
    "        for j in range(len(docs[i])):\n",
    "            inds = [a-j for a,b in enumerate(docs[i]) if b==docs[i][j]]\n",
    "            inds.remove(0)\n",
    "            is_suf = 0\n",
    "            for ind in inds:\n",
    "                if csuffix[n+ind] == 1:\n",
    "                    is_suf = 1\n",
    "            tag_list.append(is_suf)     \n",
    "            n += 1\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcsuf_LOC_train = global_csuffix_feature(docs_train,'I-LOC',csuffix_LOC_train)\n",
    "gcsuf_PER_train = global_csuffix_feature(docs_train,'I-PER',csuffix_PER_train)\n",
    "gcsuf_ORG_train = global_csuffix_feature(docs_train,'I-ORG',csuffix_ORG_train)\n",
    "gcsuf_MISC_train = global_csuffix_feature(docs_train,'I-MISC',csuffix_MISC_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcsuf_LOC_test = global_csuffix_feature(docs_test,'I-LOC',csuffix_LOC_test)\n",
    "gcsuf_PER_test = global_csuffix_feature(docs_test,'I-PER',csuffix_PER_test)\n",
    "gcsuf_ORG_test = global_csuffix_feature(docs_test,'I-ORG',csuffix_ORG_test)\n",
    "gcsuf_MISC_test = global_csuffix_feature(docs_test,'I-MISC',csuffix_MISC_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step5：训练CRF模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = [POS_tag_train,\n",
    "symbal_infor_train,\n",
    "number_infor_train,\n",
    "first_cap_train,\n",
    "first_word_train,\n",
    "all_capitals_train,\n",
    "doc_loc_train,\n",
    "fil_infor_train,\n",
    "common_word_train,\n",
    "bi_LOC_train,\n",
    "bi_PER_train, \n",
    "bi_ORG_train, \n",
    "bi_MISC_train,\n",
    "suffix_LOC_train,\n",
    "suffix_PER_train,\n",
    "suffix_ORG_train, \n",
    "suffix_MISC_train,\n",
    "csuffix_LOC_train,\n",
    "csuffix_PER_train,\n",
    "csuffix_ORG_train,\n",
    "csuffix_MISC_train,\n",
    "functional_LOC_train,\n",
    "functional_PER_train,\n",
    "functional_ORG_train,\n",
    "functional_MISC_train,\n",
    "guni_LOC_train,\n",
    "guni_PER_train,\n",
    "guni_ORG_train,\n",
    "guni_MISC_train,\n",
    "gbi_LOC_train,\n",
    "gbi_PER_train,\n",
    "gbi_ORG_train,\n",
    "gbi_MISC_train,\n",
    "gcsuf_LOC_train,\n",
    "gcsuf_PER_train,\n",
    "gcsuf_ORG_train,\n",
    "gcsuf_MISC_train]\n",
    "\n",
    "test_input = [POS_tag_test,\n",
    "symbal_infor_test,\n",
    "number_infor_test,\n",
    "first_cap_test,\n",
    "first_word_test,\n",
    "all_capitals_test,\n",
    "doc_loc_test,\n",
    "fil_infor_test,\n",
    "common_word_test,\n",
    "bi_LOC_test,\n",
    "bi_PER_test, \n",
    "bi_ORG_test, \n",
    "bi_MISC_test,\n",
    "suffix_LOC_test,\n",
    "suffix_PER_test,\n",
    "suffix_ORG_test,\n",
    "suffix_MISC_test,\n",
    "csuffix_LOC_test,\n",
    "csuffix_PER_test,\n",
    "csuffix_ORG_test,\n",
    "csuffix_MISC_test,\n",
    "functional_LOC_test,\n",
    "functional_PER_test,\n",
    "functional_ORG_test,\n",
    "functional_MISC_test,\n",
    "guni_LOC_test,\n",
    "guni_PER_test,\n",
    "guni_ORG_test,\n",
    "guni_MISC_test,\n",
    "gbi_LOC_test,\n",
    "gbi_PER_test,\n",
    "gbi_ORG_test,\n",
    "gbi_MISC_test,\n",
    "gcsuf_LOC_test,\n",
    "gcsuf_PER_test,\n",
    "gcsuf_ORG_test,\n",
    "gcsuf_MISC_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 标签\n",
    "NE_list = {}\n",
    "NE_list['O'] = 0\n",
    "NE_list['_space'] = 1\n",
    "NE_list['B-LOC'] = 2\n",
    "NE_list['I-LOC'] = 3\n",
    "NE_list['B-PER'] = 4\n",
    "NE_list['I-PER'] = 5\n",
    "NE_list['B-ORG'] = 6\n",
    "NE_list['I-ORG'] = 7\n",
    "NE_list['B-MISC'] = 8\n",
    "NE_list['I-MISC'] = 9\n",
    "def output():\n",
    "    NE_train = []\n",
    "    NE_test = []\n",
    "    for tag in df_train['NE']:\n",
    "        NE_train.append(NE_list[tag])\n",
    "    for tag_t in df_test['NE']:\n",
    "        NE_test.append(NE_list[tag_t])\n",
    "    return NE_train,NE_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 训练CRF模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
