{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 读取csv文件\n",
    "data_train_origin = []\n",
    "train = csv.reader(open('train.tsv', 'rt'))\n",
    "for row in train:\n",
    "    data_train_origin.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train_origin = data_train_origin[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 数据的分割\n",
    "data_train = []\n",
    "for item in data_train_origin:\n",
    "    if (len(item)>0):\n",
    "        for j in range(1,len(item)):\n",
    "            item[0]+=item[j]\n",
    "    data_train.append(item[0].split('\\t'))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 删除标题\n",
    "data_train.remove(data_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 简单预处理\n",
    "for i in range(len(data_train)):\n",
    "    data_train[i][0] = int(data_train[i][0])\n",
    "    data_train[i][1] = int(data_train[i][1])\n",
    "    data_train[i][2] = data_train[i][2].replace('  ',' ')\n",
    "    data_train[i][3] = int(data_train[i][3])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 切分单词\n",
    "tokenized_lines = []\n",
    "for item in data_train:\n",
    "    tokenized_lines.append(item[2].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 去除标点，替换大小写\n",
    "punctuation = [\",\", \":\", \";\", \".\", \"'\", '\"', \"’\", \"?\", \"/\", \"-\", \"+\", \"&\", \"(\", \")\"]\n",
    "clean_tokenized = []\n",
    "for item in tokenized_lines:\n",
    "    tokens = []\n",
    "    for token in item:\n",
    "        token = token.lower()\n",
    "        for punc in punctuation:\n",
    "            token = token.replace(punc, \"\")\n",
    "        tokens.append(token)\n",
    "    clean_tokenized.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 生成原始的词汇集合,剔除了只出现过一次的token\n",
    "unique_tokens = []\n",
    "single_tokens = []\n",
    "for tokens in clean_tokenized:\n",
    "    for token in tokens:\n",
    "        if token not in single_tokens:\n",
    "            single_tokens.append(token)\n",
    "        elif token in single_tokens and token not in unique_tokens:\n",
    "            unique_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 统计每个词出现的频率\n",
    "all_tokens = []\n",
    "n = 0\n",
    "for seq in clean_tokenized:\n",
    "    for word in seq:\n",
    "        all_tokens.append(word)\n",
    "        if (word not in unique_tokens):\n",
    "            n += 1\n",
    "\n",
    "dic = {}\n",
    "for token in unique_tokens:\n",
    "    dic[token] = all_tokens.count(token)\n",
    "dic['_UNK'] = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 构建哈夫曼树\n",
    "# 定义节点类\n",
    "class Node(object):\n",
    "    def __init__(self,name=None,value=None):\n",
    "        self._name=name\n",
    "        self._value=value\n",
    "        self._parent=None\n",
    "        self._left=None\n",
    "        self._right=None\n",
    "    def get_name(self):\n",
    "        return self._name\n",
    "    def get_value(self):\n",
    "        return self._value\n",
    "    def get_parent(self):\n",
    "        return self._parent\n",
    "    def get_left(self):\n",
    "        return self._left\n",
    "    def get_right(self):\n",
    "        return self._right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "节点类包含的信息——名称、值、父节点、左子节点和右子节点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#哈夫曼树类\n",
    "class HuffmanTree(object):\n",
    "    def __init__(self,frequency_dic):\n",
    "        self.a=[Node(key,frequency_dic[key]) for key in frequency_dic.keys()] #根据输入的字符及其频数生成叶子节点\n",
    "        self.leaves = []\n",
    "        while len(self.a)!=1:    \n",
    "            self.a.sort(key=lambda node:node._value,reverse=True)\n",
    "            c=Node(value=(self.a[-1]._value+self.a[-2]._value))\n",
    "            c._left=self.a[-1]\n",
    "            c._right=self.a[-2]\n",
    "            self.a[-1]._parent=c\n",
    "            self.leaves.append(self.a[-1])\n",
    "            self.a.pop(-1)\n",
    "            self.a[-1]._parent=c\n",
    "            self.leaves.append(self.a[-1])\n",
    "            self.a.pop(-1)\n",
    "            self.a.append(c)\n",
    "        self.root=self.a[0]\n",
    "        self.b=list(range(20))  #self.b用于保存每个叶子节点的Haffuman编码,range的值只需要不小于树的深度就行\n",
    "        self.dic = {}\n",
    "        \n",
    "    #用递归的思想生成编码\n",
    "    def pre(self,tree,length):\n",
    "        node=tree\n",
    "        if (not node):\n",
    "            return\n",
    "        elif node._name:\n",
    "            print(node._name + '的编码为:')\n",
    "            code = []\n",
    "            for i in range(length):\n",
    "                print(self.b[i])\n",
    "                code.append(self.b[i])\n",
    "            print('\\n')\n",
    "            self.dic[node._name] = code\n",
    "            return\n",
    "        self.b[length]=0\n",
    "        self.pre(node._left,length+1)\n",
    "        self.b[length]=1\n",
    "        self.pre(node._right,length+1)\n",
    "     #生成哈夫曼编码   \n",
    "    def get_code(self):\n",
    "        self.pre(self.root,0)\n",
    "        self.a[0]._parent = None\n",
    "        return self.leaves+self.a, self.dic\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方法是从叶子开始，反向建立整个哈夫曼树，几个注意点：\n",
    "1、 sort()函数的使用——key参数指定一个函数在排序之前调用，这里node代之之前的a中的元素，然后按照其value的属性进行排序；reverse是反转的意思，反向排序。\n",
    "2、 pop()函数的使用——删除元素，当参数非指定元素而是数值的时候，删除从0到指定序号的元素。\n",
    "3、 递归部分的设计\n",
    "参考：\n",
    "https://blog.csdn.net/lzq20115395/article/details/78906863"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of的编码为:\n",
      "0\n",
      "0\n",
      "0\n",
      "\n",
      "\n",
      "which的编码为:\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "\n",
      "\n",
      "would的编码为:\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "\n",
      "\n",
      "time的编码为:\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "\n",
      "\n",
      "hard的编码为:\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "\n",
      "\n",
      "ismail的编码为:\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "\n",
      "\n",
      "goose的编码为:\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "\n",
      "\n",
      "a的编码为:\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "\n",
      "\n",
      "none的编码为:\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "\n",
      "\n",
      "i的编码为:\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "\n",
      "\n",
      "entertaining的编码为:\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "\n",
      "\n",
      "the的编码为:\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "\n",
      "\n",
      "good的编码为:\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "\n",
      "\n",
      "demonstrating的编码为:\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "\n",
      "\n",
      "that的编码为:\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "\n",
      "\n",
      "merchant的编码为:\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "\n",
      "\n",
      "s的编码为:\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "\n",
      "\n",
      "amounts的编码为:\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "\n",
      "\n",
      "and的编码为:\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "\n",
      "\n",
      "adage的编码为:\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "\n",
      "\n",
      "to的编码为:\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "\n",
      "\n",
      "for的编码为:\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "\n",
      "\n",
      "through的编码为:\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "\n",
      "\n",
      "one的编码为:\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "\n",
      "\n",
      "_UNK的编码为:\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "\n",
      "\n",
      "sitting的编码为:\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "\n",
      "\n",
      "have的编码为:\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "\n",
      "\n",
      "introspective的编码为:\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "\n",
      "\n",
      "even的编码为:\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "\n",
      "\n",
      "series的编码为:\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "\n",
      "\n",
      "fans的编码为:\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "\n",
      "\n",
      "seeking的编码为:\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "\n",
      "\n",
      "what的编码为:\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "\n",
      "\n",
      "independent的编码为:\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "\n",
      "\n",
      "this的编码为:\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "\n",
      "\n",
      "much的编码为:\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "\n",
      "\n",
      "is的编码为:\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "\n",
      "\n",
      "gander的编码为:\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "\n",
      "\n",
      "occasionally的编码为:\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "\n",
      "\n",
      "some的编码为:\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "\n",
      "\n",
      "suspect的编码为:\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "\n",
      "\n",
      "work的编码为:\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "\n",
      "\n",
      "story的编码为:\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "\n",
      "\n",
      "escapades的编码为:\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "\n",
      "\n",
      "quiet的编码为:\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "\n",
      "\n",
      "also的编码为:\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "\n",
      "\n",
      "worth的编码为:\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "\n",
      "\n",
      "amuses的编码为:\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "\n",
      "\n",
      "but的编码为:\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 获取哈夫曼树\n",
    "tree= HuffmanTree(dic)\n",
    "leaves_list,leaves_code = tree.get_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 将start和end加入到token中\n",
    "unique_tokens.append('_STA')\n",
    "unique_tokens.append('_END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 对词汇表进行编号，并生成反向词典\n",
    "vocab = {}\n",
    "for i in range(len(unique_tokens)):\n",
    "    vocab[unique_tokens[i]] = i\n",
    "\n",
    "reversed_vocab = {value:key for key,value in vocab.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正向词典是通过词索引编号，反向词典是通过编号索引词汇，在抽取哈夫曼树枝的时候会用到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 定义抽取词汇序号的函数\n",
    "def get_index(word):\n",
    "    if (word in unique_tokens):\n",
    "        ind = vocab[word]\n",
    "    else:\n",
    "        ind = vocab['_UNK']\n",
    "    return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 表征文本中的每个词汇。\n",
    "representations = []\n",
    "words = []\n",
    "for seq in clean_tokenized:\n",
    "    lines = []\n",
    "    lines_words = []\n",
    "    for i in range(len(seq)):\n",
    "        context = []\n",
    "        context_words = []\n",
    "        if (i == 0):\n",
    "            context.append(get_index('_STA'))\n",
    "            context.append(get_index('_STA'))\n",
    "            context_words.append('_STA')\n",
    "            context_words.append('_STA')\n",
    "        elif (i == 1):\n",
    "            context.append(get_index('_STA'))\n",
    "            context.append(get_index(seq[i-1]))\n",
    "            context_words.append('_STA')\n",
    "            context_words.append(seq[i-1])\n",
    "        else:\n",
    "            context.append(get_index(seq[i-2]))\n",
    "            context.append(get_index(seq[i-1]))\n",
    "            context_words.append(seq[i-2])\n",
    "            context_words.append(seq[i-1])\n",
    "        \n",
    "        if (i == len(seq)-1):\n",
    "            context.append(get_index('_END'))\n",
    "            context.append(get_index('_END'))\n",
    "            context_words.append('_END')\n",
    "            context_words.append('_END')\n",
    "        elif (i == len(seq)-2):\n",
    "            context.append(get_index(seq[i+1]))\n",
    "            context.append(get_index('_END'))\n",
    "            context_words.append(seq[i+1])\n",
    "            context_words.append('_END')\n",
    "        else:\n",
    "            context.append(get_index(seq[i+1]))\n",
    "            context.append(get_index(seq[i+2])) \n",
    "            context_words.append(seq[i+1])\n",
    "            context_words.append(seq[i+2])\n",
    "        lines.append([context,get_index(seq[i])])\n",
    "        lines_words.append([context_words,seq[i]])\n",
    "    representations.append(lines)\n",
    "    words.append(lines_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选取2c = 4作为windows的小，前后各抽取两个词,称为Context(x),每个词汇都抽成[context(x),x]的形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 随机抽取元素进行训练，适用于SDG\n",
    "import random\n",
    "def random_sample(representations,words):\n",
    "    index = random.sample(range(len(representations)),1)[0]\n",
    "    sub_index = random.sample(range(len(representations[index])),1)[0]\n",
    "    return representations[index][sub_index][0],representations[index][sub_index][1],words[index][sub_index][0],words[index][sub_index][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 初始化参数，对应每个节点向上的树枝定义W和b\n",
    "W = []\n",
    "b = []\n",
    "for j in range(len(leaves_list)-1):\n",
    "    W.append(np.random.rand(128)/10)\n",
    "    b.append(np.random.rand()/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 对词汇表进行初始化表征，选取128维向量表征每个词\n",
    "lookup = list(np.random.rand(len(unique_tokens),128)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 训练\n",
    "n = 0\n",
    "m = 0\n",
    "while (n<100):\n",
    "    e_W = 0\n",
    "    e_b = 0\n",
    "    \n",
    "    ## 获取随机输入\n",
    "    x_context_rep, x_word_rep, x_context, x_word = random_sample(representations,words)\n",
    "    \n",
    "    ## 索引哈夫曼树节点和编码\n",
    "    # 索引编码\n",
    "    if (x_word in leaves_code.values()):\n",
    "        x_code = leaves_code[x_word]\n",
    "    else:\n",
    "        x_code = leaves_code['_UNK']\n",
    "\n",
    "    # 索引节点\n",
    "    x_nodes = []\n",
    "    x_nodes_index = []\n",
    "    if ([x for x in leaves_list if x.get_name() == x_word] == []):\n",
    "        node = [x for x in leaves_list if x.get_name() == '_UNK'][0]\n",
    "    else:\n",
    "        node = [x for x in leaves_list if x.get_name() == x_word][0]\n",
    "    while (node.get_parent()):   \n",
    "        x_nodes.append(node)\n",
    "        x_nodes_index.append(leaves_list.index(node))\n",
    "        node = node.get_parent()\n",
    "    x_nodes.reverse()\n",
    "    x_nodes_index.reverse()\n",
    "    \n",
    "    ## word embedding\n",
    "    x_context_embedding = []\n",
    "    for item in x_context_rep:\n",
    "        x_context_embedding.append(lookup[item])\n",
    "    x_word_embedding = lookup[x_word_rep]\n",
    "    x_w = np.average(x_context_embedding,axis = 0)\n",
    "    \n",
    "    \n",
    "    ## 一次迭代更新\n",
    "    for j in range(len(x_nodes_index)):\n",
    "        f = 1/(1+np.exp(-np.matmul(x_w,W[x_nodes_index[j]])-b[x_nodes_index[j]]))\n",
    "        g = (1- x_code[j] - f)*0.1\n",
    "        e_W = e_W + g* W[x_nodes_index[j]]\n",
    "        e_b = e_b + g* b[x_nodes_index[j]]\n",
    "        dW = W[x_nodes_index[j]] + g * x_w.T\n",
    "        W[x_nodes_index[j]] = dW\n",
    "        db = b[x_nodes_index[j]] + g\n",
    "        b[x_nodes_index[j]] = db\n",
    "        \n",
    "    \n",
    "    for i in range(len(x_context_rep)):\n",
    "        dlookup = lookup[x_context_rep[i]] + e_W\n",
    "        lookup[x_context_rep[i]] = dlookup\n",
    "        \n",
    "    \n",
    "    if(np.sum(e_W**2) < 0.000001):\n",
    "        m += 1\n",
    "    \n",
    "    if(m >=10):\n",
    "        break\n",
    "    \n",
    "\n",
    "    n += 1\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "节点的索引通过递归实现，需要返回节点在列表中的索引号，之后需要对参数进行索引。反转实现节点从根到叶排列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "在CBOW中，沿着左子树走，那么就是负类(霍夫曼树编码1)，沿着右子树走，那么就是正类(霍夫曼树编码0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入：训练样本，词向量的维度大小，上下文大小2c,步长\n",
    "输出：霍夫曼树的内部节点模型参数，所有的词向量w\n",
    "1. 基于语料训练样本建立霍夫曼树。\n",
    "2. 随机初始化所有的模型参数θ，所有的词向量w。\n",
    "3. 进行梯度上升迭代过程，对于训练集中的每一个样本(context(w),w)(context(w),w)做如下处理：\n",
    "    a)  e=0， 通过2c个context的平均值表征w；\n",
    "    b)  对于霍夫曼树每层结构进行计算迭代，遵循的原则是保证按照树上编码的路径返回根节点，通过梯度上升法来不断更新分叉上的参数；\n",
    "    c)  对所有2c个词向量进行更新；\n",
    "    d)  如果梯度收敛，则结束梯度迭代，否则回到步骤3继续迭代。\n",
    "参考：http://www.cnblogs.com/pinard/p/7243513.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 通过CBOW更新后的embedding矩阵对输入数据进行表征\n",
    "x_origin = []\n",
    "for seq in clean_tokenized:\n",
    "    reps = []\n",
    "    for word in seq:\n",
    "        reps.append(lookup[get_index(word)])\n",
    "    x_origin.append(reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "## 计算每个句子的长度并存储\n",
    "lens = []\n",
    "for words in clean_tokenized:\n",
    "    lens.append(len(words))\n",
    "print(max(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 将每个句子补全到最大长度,对于padding的部分通过全0向量来补充\n",
    "x = []\n",
    "for reps in x_origin:\n",
    "    if len(reps) < max(lens):\n",
    "        for i in range(max(lens)-len(reps)):\n",
    "            reps.append(np.zeros(128))\n",
    "    x.append(reps)\n",
    "x = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 定义y并且生成one-hot vector\n",
    "y = []\n",
    "for item in data_train:\n",
    "    y.append(item[3])\n",
    "classes = np.max(y) + 1\n",
    "y = np.eye(classes)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 划分集合\n",
    "n_train = int(len(x)*0.8)\n",
    "n_test = int(len(x)*0.1)\n",
    "n_dev = int(len(x) - n_train -n_test)\n",
    "\n",
    "import random\n",
    "index_train = random.sample(range(len(x)),n_train)\n",
    "index_dev = random.sample(list(set(range(len(x)))^set(index_train)) ,n_dev)\n",
    "index_test = list(set(list(set(range(len(x)))^set(index_train)))^set(index_dev)) \n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "lens_train = []\n",
    "for index in index_train:\n",
    "    x_train.append(x[index])\n",
    "    y_train.append(y[index])\n",
    "    lens_train.append(lens[index])\n",
    "\n",
    "x_dev = []\n",
    "y_dev = []\n",
    "lens_dev = []\n",
    "for index in index_dev:\n",
    "    x_dev.append(x[index])\n",
    "    y_dev.append(y[index])\n",
    "    lens_dev.append(lens[index])\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "lens_test = []\n",
    "for index in index_test:\n",
    "    x_test.append(x[index])\n",
    "    y_test.append(y[index])\n",
    "    lens_test.append(lens[index])\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_dev = np.array(x_dev)\n",
    "y_dev = np.array(y_dev)\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 定义一个随机函数，随机抽取\n",
    "import random\n",
    "def random_sample(x,y,lens_):\n",
    "    index = random.sample(range(len(x)),1)[0]\n",
    "    balanced_m = np.concatenate((np.ones((lens_[index], 5),dtype = np.float32),np.zeros((max(lens)-lens_[index], 5),dtype = np.float32)))\n",
    "    return x[index],y[index],balanced_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## RNN模型\n",
    "# 定义输入和输出,以及一个平衡矩阵\n",
    "x_input = tf.placeholder(tf.float32,[max(lens),128])\n",
    "y_label = tf.placeholder(tf.float32,[5])\n",
    "balanced_matrix = tf.placeholder(tf.float32,[max(lens),5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义每一个输入单元，构建双层RNN结构\n",
    "cell = tf.nn.rnn_cell.BasicLSTMCell(128,state_is_tuple=True)\n",
    "multi_cell = tf.nn.rnn_cell.MultiRNNCell([cell]*2,state_is_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 初始化，每次输入一个数据\n",
    "state = multi_cell.zero_state(1, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LSTMStateTuple' object has no attribute 'get_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-150-af6926ff556b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtime_step\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreuse_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[1;33m(\u001b[0m\u001b[0mcell_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda2\\envs\\python3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope)\u001b[0m\n\u001b[1;32m    178\u001b[0m       with vs.variable_scope(vs.get_variable_scope(),\n\u001b[1;32m    179\u001b[0m                              custom_getter=self._rnn_get_variable):\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNNCell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_rnn_get_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda2\\envs\\python3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[1;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[1;31m# Apply activity regularization.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda2\\envs\\python3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, state)\u001b[0m\n\u001b[1;32m    399\u001b[0m       \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_or_size_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m     \u001b[0mconcat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_linear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_units\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[1;31m# i = input_gate, j = new_input, f = forget_gate, o = output_gate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda2\\envs\\python3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\u001b[0m in \u001b[0;36m_linear\u001b[0;34m(args, output_size, bias, bias_initializer, kernel_initializer)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[1;31m# Calculate the total size of arguments on dimension 1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0mtotal_arg_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda2\\envs\\python3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[1;31m# Calculate the total size of arguments on dimension 1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0mtotal_arg_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LSTMStateTuple' object has no attribute 'get_shape'"
     ]
    }
   ],
   "source": [
    "## 建立RNN序列\n",
    "outputs = []\n",
    "with tf.variable_scope(\"RNN\"):\n",
    "    for time_step in range(max(lens)):\n",
    "        if time_step > 0: \n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "        (cell_output,state) = cell(x_input[time_step, :],state)\n",
    "        outputs.append(cell_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'get_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-469db7aad42f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'get_shape'"
     ]
    }
   ],
   "source": [
    "x.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
