{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 读取csv文件\n",
    "data_train_origin = []\n",
    "train = csv.reader(open('train.tsv', 'rt'))\n",
    "for row in train:\n",
    "    data_train_origin.append(row)\n",
    "\n",
    "data_test_origin = []\n",
    "train = csv.reader(open('test.tsv', 'rt'))\n",
    "for row in train:\n",
    "    data_test_origin.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train_origin = data_train_origin[0:1000]\n",
    "data_test_origin = data_test_origin[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 数据的分割\n",
    "data_train = []\n",
    "for item in data_train_origin:\n",
    "    if (len(item)>0):\n",
    "        for j in range(1,len(item)):\n",
    "            item[0]+=item[j]\n",
    "    data_train.append(item[0].split('\\t'))\n",
    "\n",
    "data_test = []\n",
    "for item in data_test_origin:\n",
    "    if (len(item)>0):\n",
    "        for j in range(1,len(item)):\n",
    "            item[0]+=item[j]\n",
    "    data_test.append(item[0].split('\\t'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 删除标题\n",
    "data_train.remove(data_train[0])\n",
    "data_test.remove(data_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 简单预处理\n",
    "for i in range(len(data_train)):\n",
    "    data_train[i][0] = int(data_train[i][0])\n",
    "    data_train[i][1] = int(data_train[i][1])\n",
    "    data_train[i][2] = data_train[i][2].replace('  ',' ')\n",
    "    data_train[i][3] = int(data_train[i][3])\n",
    "    \n",
    "for i in range(len(data_test)):\n",
    "    data_test[i][0] = int(data_test[i][0])\n",
    "    data_test[i][1] = int(data_test[i][1])\n",
    "    data_test[i][2] = data_test[i][2].replace('  ',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 切分单词\n",
    "tokenized_lines = []\n",
    "for item in data_train:\n",
    "    tokenized_lines.append(item[2].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 去除标点，替换大小写\n",
    "punctuation = [',', ':', ';', '.', \"'\", '\"', \"’\", '?', '/', '-', '+', '&', '(', ')']\n",
    "clean_tokenized = []\n",
    "for item in tokenized_lines:\n",
    "    tokens = []\n",
    "    for token in item:\n",
    "        token = token.lower()\n",
    "        for punc in punctuation:\n",
    "            token = token.replace(punc,' ')\n",
    "        tokens.append(token)\n",
    "    clean_tokenized.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 读入GloVe所定义的词向量（100维度）\n",
    "f = open(\"glove.6B.100d.txt\",'r', encoding='UTF-8')  \n",
    "lines = f.readlines()  \n",
    "f.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 对于读入的字典进行预处理，处理成字典的格式\n",
    "GloVe_dic = {}\n",
    "for line in lines:\n",
    "    line = line.replace('\\n','')\n",
    "    line = line.split(\" \")\n",
    "    value = []\n",
    "    for i in range(1,len(line)):\n",
    "        value.append(float(line[i]))\n",
    "    GloVe_dic[line[0]] = value\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "## 计算每个句子的长度并存储\n",
    "lens = []\n",
    "for words in clean_tokenized:\n",
    "    lens.append(len(words))\n",
    "print(max(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 将每个句子表征称为index的形式，并补全到最大长度\n",
    "representations_origin = []\n",
    "for words in clean_tokenized:\n",
    "    if len(words) < max(lens):\n",
    "        for i in range(max(lens)-len(words)):\n",
    "            words.append('_PAD')       \n",
    "    representations_origin.append(words)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 对于每个单词通过字典中的representations来表征\n",
    "representations = []\n",
    "for seq in representations_origin:\n",
    "    representation = []\n",
    "    for word in seq:\n",
    "        if (word in GloVe_dic.keys()):\n",
    "            representation.append(GloVe_dic[word])\n",
    "        else:\n",
    "            representation.append(list(np.zeros(100)))\n",
    "    representations.append(representation)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OOV通过全0向量来表征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 定义x，y\n",
    "x = representations\n",
    "y = []\n",
    "for item in data_train:\n",
    "    y.append(item[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## y生成one-hot vector\n",
    "classes = np.max(y) + 1\n",
    "y = np.eye(classes)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 划分集合\n",
    "n_train = int(len(x)*0.8)\n",
    "n_test = int(len(x)*0.1)\n",
    "n_dev = int(len(x) - n_train -n_test)\n",
    "\n",
    "import random\n",
    "index_train = random.sample(range(len(x)),n_train)\n",
    "index_dev = random.sample(list(set(range(len(x)))^set(index_train)) ,n_dev)\n",
    "index_test = list(set(list(set(range(len(x)))^set(index_train)))^set(index_dev)) \n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "lens_train = []\n",
    "for index in index_train:\n",
    "    x_train.append(x[index])\n",
    "    y_train.append(y[index])\n",
    "    lens_train.append(lens[index])\n",
    "\n",
    "x_dev = []\n",
    "y_dev = []\n",
    "lens_dev = []\n",
    "for index in index_dev:\n",
    "    x_dev.append(x[index])\n",
    "    y_dev.append(y[index])\n",
    "    lens_dev.append(lens[index])\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "lens_test = []\n",
    "for index in index_test:\n",
    "    x_test.append(x[index])\n",
    "    y_test.append(y[index])\n",
    "    lens_test.append(lens[index])\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_dev = np.array(x_dev)\n",
    "y_dev = np.array(y_dev)\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 定义一个随机函数，随机抽取\n",
    "import random\n",
    "def random_sample(x,y,lens_):\n",
    "    index = random.sample(range(len(x)),1)[0]\n",
    "    balanced_m = np.concatenate((np.ones((lens_[index], 5),dtype = np.float32),np.zeros((max(lens)-lens_[index], 5),dtype = np.float32)))\n",
    "    return x[index],y[index],balanced_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 通过tf构建logistic model\n",
    "# 定义输入和输出,以及一个平衡矩阵\n",
    "x_input = tf.placeholder(tf.float32,[max(lens),100])\n",
    "y_label = tf.placeholder(tf.float32,[5])\n",
    "balanced_matrix = tf.placeholder(tf.float32,[max(lens),5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义参数\n",
    "W = tf.Variable(tf.random_normal([100,5],stddev=1))\n",
    "b = tf.Variable(tf.random_normal([1,5],stddev=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 计算预测值y_,通过对每个词的预测求平均获得对于整个文本的情感分析\n",
    "y_pre = tf.reduce_mean(tf.nn.sigmoid(tf.matmul(x_input,W)+b)*balanced_matrix,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 计算loss function\n",
    "L = -tf.reduce_mean(y_label*tf.log(y_pre) + (1-y_label)*tf.log(1-y_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 梯度下降法\n",
    "train_step = tf.train.GradientDescentOptimizer(0.000001).minimize(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 初始化variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iteration:  0\n",
      "2 iteration:  0\n",
      "3 iteration:  0\n",
      "4 iteration:  1\n",
      "5 iteration:  2\n",
      "6 iteration:  0\n",
      "7 iteration:  1\n",
      "8 iteration:  0\n",
      "9 iteration:  1\n",
      "10 iteration:  2\n",
      "11 iteration:  0\n",
      "12 iteration:  1\n",
      "13 iteration:  2\n",
      "14 iteration:  3\n",
      "15 iteration:  0\n",
      "16 iteration:  1\n",
      "17 iteration:  0\n",
      "18 iteration:  0\n",
      "19 iteration:  1\n",
      "20 iteration:  0\n",
      "21 iteration:  0\n",
      "22 iteration:  1\n",
      "23 iteration:  2\n",
      "24 iteration:  0\n",
      "25 iteration:  1\n",
      "26 iteration:  0\n",
      "27 iteration:  0\n",
      "28 iteration:  1\n",
      "29 iteration:  0\n",
      "30 iteration:  0\n",
      "31 iteration:  1\n",
      "32 iteration:  0\n",
      "33 iteration:  1\n",
      "34 iteration:  0\n",
      "35 iteration:  1\n",
      "36 iteration:  0\n",
      "37 iteration:  1\n",
      "38 iteration:  0\n",
      "39 iteration:  0\n",
      "40 iteration:  0\n",
      "41 iteration:  1\n",
      "42 iteration:  2\n",
      "43 iteration:  0\n",
      "44 iteration:  0\n",
      "45 iteration:  0\n",
      "46 iteration:  1\n",
      "47 iteration:  0\n",
      "48 iteration:  1\n",
      "49 iteration:  0\n",
      "50 iteration:  0\n",
      "51 iteration:  1\n",
      "52 iteration:  0\n",
      "53 iteration:  1\n",
      "54 iteration:  2\n",
      "55 iteration:  0\n",
      "56 iteration:  0\n",
      "57 iteration:  1\n",
      "58 iteration:  2\n",
      "59 iteration:  0\n",
      "60 iteration:  1\n",
      "61 iteration:  2\n",
      "62 iteration:  0\n",
      "63 iteration:  1\n",
      "64 iteration:  0\n",
      "65 iteration:  1\n",
      "66 iteration:  0\n",
      "67 iteration:  0\n",
      "68 iteration:  0\n",
      "69 iteration:  1\n",
      "70 iteration:  0\n",
      "71 iteration:  1\n",
      "72 iteration:  2\n",
      "73 iteration:  0\n",
      "74 iteration:  1\n",
      "75 iteration:  2\n",
      "76 iteration:  3\n",
      "77 iteration:  0\n",
      "78 iteration:  0\n",
      "79 iteration:  1\n",
      "80 iteration:  2\n",
      "81 iteration:  0\n",
      "82 iteration:  1\n",
      "83 iteration:  0\n",
      "84 iteration:  0\n",
      "85 iteration:  1\n",
      "86 iteration:  0\n",
      "87 iteration:  1\n",
      "88 iteration:  0\n",
      "89 iteration:  0\n",
      "90 iteration:  1\n",
      "91 iteration:  2\n",
      "92 iteration:  0\n",
      "93 iteration:  0\n",
      "94 iteration:  1\n",
      "95 iteration:  2\n",
      "96 iteration:  0\n",
      "97 iteration:  1\n",
      "98 iteration:  0\n",
      "99 iteration:  0\n",
      "100 iteration:  1\n"
     ]
    }
   ],
   "source": [
    "# 进行训练\n",
    "L_train = []\n",
    "L_dev = []\n",
    "n = 0\n",
    "m = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    while (n < 20 and m< 100):\n",
    "        x_data,y_data,balanced_m = random_sample(x_train,y_train,lens_train)      \n",
    "        x_dev_data,y_dev_data,balanced_dev_m = random_sample(x_dev,y_dev,lens_dev)\n",
    "\n",
    "        sess.run(train_step,feed_dict={x_input:x_data,y_label:y_data,balanced_matrix:balanced_m})\n",
    "        L_dev.append(sess.run(L,feed_dict={x_input:x_dev_data,y_label:y_dev_data,balanced_matrix:balanced_dev_m}))\n",
    "        L_train.append(sess.run(L,feed_dict={x_input:x_data,y_label:y_data,balanced_matrix:balanced_m}))\n",
    "        m += 1\n",
    "        if (len(L_dev)>=2):\n",
    "            if (L_dev[-1] > L_dev[-2]):\n",
    "                n += 1\n",
    "            else:\n",
    "                n = 0\n",
    "        print (str(m)+' iteration: ',n)\n",
    "        \n",
    "    W_ = sess.run(W)\n",
    "    b_ = sess.run(b)\n",
    "    y_test_pre = []\n",
    "    for j in range(len(x_test)):\n",
    "        balanced_test_m = np.concatenate((np.ones((lens_test[j], 5),dtype = np.float32),np.zeros((max(lens)-lens_test[j], 5),dtype = np.float32)))\n",
    "        \n",
    "        y_test_pre.append(sess.run(y_pre,feed_dict={x_input:x_test[j],y_label:y_test[j],balanced_matrix:balanced_test_m}))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 设计模型在test set上的验证\n",
    "def evaluate(W,b,x_test,y_test):\n",
    "    y = []\n",
    "    for onehot in y_test:\n",
    "        y.append(np.where(onehot > 0.5)[0][0])\n",
    "    y_pre  = np.average(1/(1+np.exp(-np.dot(x_test,W)-b)),axis=1)\n",
    "    y_ = []\n",
    "    for vector in y_pre:\n",
    "        y_.append(np.where(vector == max(vector))[0][0])\n",
    "    loss = -np.average(y_test*np.log(y_pre)+(1-y_test)*np.log(1-y_pre))\n",
    "    accuracy = float(np.sum(np.array(y)==np.array(y_)))/float(len(y))\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    elems = list(set(y))\n",
    "    for elem in elems:\n",
    "        p = 0\n",
    "        r = 0\n",
    "        n = 0\n",
    "        for i in range(len(y)):\n",
    "            if (y[i] == elem):\n",
    "                p += 1\n",
    "            if (y_[i] == elem):\n",
    "                r += 1\n",
    "            if (y[i] == y_[i] and y[i] == elem):\n",
    "                n +=1\n",
    "        if (p != 0):\n",
    "            precision.append(float(n)/float(p))\n",
    "        else:\n",
    "            precision.append(0)\n",
    "        if (r != 0):\n",
    "            recall.append(float(n)/float(r))\n",
    "        else:\n",
    "            recall.append(0)\n",
    "    for j in range(len(precision)):\n",
    "        if (precision[j]!=0 or recall[j] != 0):\n",
    "            f1.append(2*precision[j]*recall[j]/(precision[j]+recall[j]))\n",
    "        else:\n",
    "            f1.append(0)\n",
    "    return loss,accuracy,elems,precision,recall,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "loss,accuracy,elems,precision,recall,f1 = evaluate(W_,b_,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5717874967868759"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6363636363636364"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 1.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0, 0.6428571428571429, 0, 0]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0.782608695652174, 0, 0]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
