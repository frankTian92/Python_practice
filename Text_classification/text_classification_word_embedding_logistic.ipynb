{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 读取csv文件\n",
    "data_train_origin = []\n",
    "train = csv.reader(open('train.tsv', 'rt'))\n",
    "for row in train:\n",
    "    data_train_origin.append(row)\n",
    "    \n",
    "data_test_origin = []\n",
    "train = csv.reader(open('test.tsv', 'rt'))\n",
    "for row in train:\n",
    "    data_test_origin.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 数据的分割\n",
    "data_train = []\n",
    "for item in data_train_origin:\n",
    "    if (len(item)>0):\n",
    "        for j in range(1,len(item)):\n",
    "            item[0]+=item[j]\n",
    "    data_train.append(item[0].split('\\t'))\n",
    "    \n",
    "data_test = []\n",
    "for item in data_test_origin:\n",
    "    if (len(item)>0):\n",
    "        for j in range(1,len(item)):\n",
    "            item[0]+=item[j]\n",
    "    data_test.append(item[0].split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 删除标题\n",
    "data_train.remove(data_train[0])\n",
    "data_test.remove(data_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 简单预处理\n",
    "for i in range(len(data_train)):\n",
    "    data_train[i][0] = int(data_train[i][0])\n",
    "    data_train[i][1] = int(data_train[i][1])\n",
    "    data_train[i][2] = data_train[i][2].replace('  ',' ')\n",
    "    data_train[i][3] = int(data_train[i][3])\n",
    "    \n",
    "for i in range(len(data_test)):\n",
    "    data_test[i][0] = int(data_test[i][0])\n",
    "    data_test[i][1] = int(data_test[i][1])\n",
    "    data_test[i][2] = data_test[i][2].replace('  ',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 切分单词\n",
    "tokenized_lines = []\n",
    "for item in data_train:\n",
    "    tokenized_lines.append(item[2].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 去除标点，替换大小写\n",
    "punctuation = [',', ':', ';', '.', \"'\", '\"', \"’\", '?', '/', '-', '+', '&', '(', ')']\n",
    "clean_tokenized = []\n",
    "for item in tokenized_lines:\n",
    "    tokens = []\n",
    "    for token in item:\n",
    "        token = token.lower()\n",
    "        for punc in punctuation:\n",
    "            token = token.replace(punc, \" \")\n",
    "        tokens.append(token)\n",
    "    clean_tokenized.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 生成原始的词汇集合,剔除了只出现过一次的token\n",
    "unique_tokens = []\n",
    "single_tokens = []\n",
    "for tokens in clean_tokenized:\n",
    "    for token in tokens:\n",
    "        if token not in single_tokens:\n",
    "            single_tokens.append(token)\n",
    "        elif token in single_tokens and token not in unique_tokens:\n",
    "            unique_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 加入起止符号和OOV\n",
    "unique_tokens.append('_UNK')\n",
    "unique_tokens.append('_PAD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 对每个tokens进行编号\n",
    "dic = {}\n",
    "for i in range(len(unique_tokens)):\n",
    "    dic[unique_tokens[i]]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "## 计算每个句子的长度并存储\n",
    "lens = []\n",
    "for words in clean_tokenized:\n",
    "    lens.append(len(words))\n",
    "print(max(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 将每个句子表征称为index的形式，并补全到最大长度\n",
    "representations = []\n",
    "for words in clean_tokenized:\n",
    "    reps = []\n",
    "    if len(words) < max(lens):\n",
    "        for i in range(max(lens)-len(words)):\n",
    "            words.append('_PAD')       \n",
    "    for word in words:\n",
    "        if (word in unique_tokens):\n",
    "            reps.append(dic[word])\n",
    "        else:\n",
    "            reps.append(dic['_UNK'])\n",
    "    representations.append(reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 定义x，y\n",
    "x = representations\n",
    "y = []\n",
    "for item in data_train:\n",
    "    y.append(item[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## y生成one-hot vector\n",
    "classes = np.max(y) + 1\n",
    "y = np.eye(classes)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 划分集合\n",
    "n_train = int(len(x)*0.8)\n",
    "n_test = int(len(x)*0.1)\n",
    "n_dev = int(len(x) - n_train -n_test)\n",
    "\n",
    "import random\n",
    "index_train = random.sample(range(len(x)),n_train)\n",
    "index_dev = random.sample(list(set(range(len(x)))^set(index_train)) ,n_dev)\n",
    "index_test = list(set(list(set(range(len(x)))^set(index_train)))^set(index_dev)) \n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "lens_train = []\n",
    "for index in index_train:\n",
    "    x_train.append(x[index])\n",
    "    y_train.append(y[index])\n",
    "    lens_train.append(lens[index])\n",
    "\n",
    "x_dev = []\n",
    "y_dev = []\n",
    "lens_dev = []\n",
    "for index in index_dev:\n",
    "    x_dev.append(x[index])\n",
    "    y_dev.append(y[index])\n",
    "    lens_dev.append(lens[index])\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "lens_test = []\n",
    "for index in index_test:\n",
    "    x_test.append(x[index])\n",
    "    y_test.append(y[index])\n",
    "    lens_test.append(lens[index])\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_dev = np.array(x_dev)\n",
    "y_dev = np.array(y_dev)\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 定义一个随机函数，随机抽取\n",
    "import random\n",
    "def random_sample(x,y,lens_):\n",
    "    index = random.sample(range(len(x)),1)[0]\n",
    "    balanced_m = np.concatenate((np.ones((lens_[index], 5),dtype = np.float32),np.zeros((max(lens)-lens_[index], 5),dtype = np.float32)))\n",
    "    return x[index],y[index],balanced_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用于随机梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 通过tf构建logistic model\n",
    "# 定义输入和输出,以及一个平衡矩阵\n",
    "x_input = tf.placeholder(tf.int32,[max(lens)])\n",
    "y_label = tf.placeholder(tf.float32,[5])\n",
    "balanced_matrix = tf.placeholder(tf.float32,[max(lens),5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "平衡矩阵是用来补偿padding所带来的影响，通过对于padding单词计算损失函数时全0处理来减小它们在训练时的作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义word embedding层\n",
    "lookup = tf.Variable(tf.random_normal([len(unique_tokens),128],stddev=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义参数\n",
    "W = tf.Variable(tf.random_normal([128,5],stddev=1))\n",
    "b = tf.Variable(tf.random_normal([1,5],stddev=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算embedding lookup\n",
    "x_embedding = tf.nn.embedding_lookup(lookup,x_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现embedding lookup的代码：可以通过tf.nn.embedding_lookup(lookup matrix, input_index)实现\n",
    "难点：tensor不能迭代，因此先通过一次session计算出x，再通过迭代计算每条句子所表征的矩阵\n",
    "使用随机梯度下降法，每次学习一个样本，可以避免这个问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算预测值y_,通过对每个词的预测求平均获得对于整个文本的情感分析\n",
    "y_pre = tf.reduce_mean(tf.nn.sigmoid(tf.matmul(x_embedding,W)+b)*balanced_matrix,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于每个embedding的输入，通过sigmoid函数计算预测向量. PS:\n",
    "1、 当一个句子文本长度小于最大长度时，需要对去除padding部分对于预测的影响，通过构造一个只包含0，1的矩阵，当行数小于文本长度的时候，值为1；当大于文本长度但小于最大长度时，值为0，再将两个矩阵对应相乘（不是点积）得到新的矩阵。实现方法：np.ones(shape,dtype), np.zeros(shape,dtype), np.concatenate((a,b))实现。\n",
    "2、 对于矩阵表示一个文本中每个词的情感情况，通过求平均值来表征句子的情感分类。实现方法：tf.reduce_mean()通过设置axis参数来调整形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算loss function\n",
    "L = -tf.reduce_mean(y_label*tf.log(y_pre) + (1-y_label)*tf.log(1-y_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 梯度下降法\n",
    "train_step = tf.train.GradientDescentOptimizer(0.000001).minimize(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 初始化variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 进行训练\n",
    "L_train = []\n",
    "L_dev = []\n",
    "n = 0\n",
    "m = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    while (n < 20 and m< 10000):\n",
    "        x_data,y_data,balanced_m = random_sample(x_train,y_train,lens_train)      \n",
    "        x_dev_data,y_dev_data,balanced_dev_m = random_sample(x_dev,y_dev,lens_dev)\n",
    "\n",
    "        sess.run(train_step,feed_dict={x_input:x_data,y_label:y_data,balanced_matrix:balanced_m})\n",
    "        L_dev.append(sess.run(L,feed_dict={x_input:x_dev_data,y_label:y_dev_data,balanced_matrix:balanced_dev_m}))\n",
    "        L_train.append(sess.run(L,feed_dict={x_input:x_data,y_label:y_data,balanced_matrix:balanced_m}))\n",
    "        m += 1\n",
    "        if (len(L_dev)>=2):\n",
    "            if (L_dev[-1] > L_dev[-2]):\n",
    "                n += 1\n",
    "            else:\n",
    "                n = 0\n",
    "        print (str(m)+' iteration: ',n)\n",
    "        \n",
    "    W_ = sess.run(W)\n",
    "    b_ = sess.run(b)\n",
    "    lookup_ = sess.run(lookup)\n",
    "    y_test_pre = []\n",
    "    for j in range(len(x_test)):\n",
    "        balanced_test_m = np.concatenate((np.ones((lens_test[j], 5),dtype = np.float32),np.zeros((max(lens)-lens_test[j], 5),dtype = np.float32)))\n",
    "        y_test_pre.append(sess.run(y_pre,feed_dict={x_input:x_test[j],y_label:y_test[j],balanced_matrix:balanced_test_m}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SDG：记录每次迭代后的train set和dev set的L值，直到dev set的L值不再减小或者迭代10000次为止"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 设计模型在test set上的验证\n",
    "def evaluate(y_test_pre,y_test):    \n",
    "    y = []\n",
    "    y_ = []\n",
    "    for onehot in y_test:\n",
    "        y.append(np.where(onehot > 0.5)[0][0])    \n",
    "    for vector in y_test_pre:\n",
    "        y_.append(np.where(vector == max(vector))[0][0])\n",
    "    y = np.array(y)\n",
    "    y_ = np.array(y_)\n",
    "    loss = -np.average(y_test*np.log(y_test_pre)+(1-np.array(y_test))*np.log(1-np.array(y_test_pre)))\n",
    "    accuracy = float(np.sum(np.array(y)==np.array(y_)))/float(len(y))\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    elems = list(set(y))\n",
    "    for elem in elems:\n",
    "        p = 0\n",
    "        r = 0\n",
    "        n = 0\n",
    "        for i in range(len(y)):\n",
    "            if (y[i] == elem):\n",
    "                p += 1\n",
    "            if (y_[i] == elem):\n",
    "                r += 1\n",
    "            if (y[i] == y_[i] and y[i] == elem):\n",
    "                n +=1\n",
    "        if (p != 0):\n",
    "            precision.append(float(n)/float(p))\n",
    "        else:\n",
    "            precision.append(0)\n",
    "        if (r != 0):\n",
    "            recall.append(float(n)/float(r))\n",
    "        else:\n",
    "            recall.append(0)\n",
    "    for j in range(len(precision)):\n",
    "        if (precision[j]!=0 or recall[j] != 0):\n",
    "            f1.append(2*precision[j]*recall[j]/(precision[j]+recall[j]))\n",
    "        else:\n",
    "            f1.append(0)\n",
    "    return loss,accuracy,elems,precision,recall,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "loss,accuracy,elems,precision,recall,f1 = evaluate(y_test_pre,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0589316942770737"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2653061224489796"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.42857142857142855, 0.1724137931034483, 0.3333333333333333, 0.0]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2222222222222222, 0.2, 0.7142857142857143, 0.375, 0.0]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3636363636363636,\n",
       " 0.27272727272727276,\n",
       " 0.2777777777777778,\n",
       " 0.35294117647058826,\n",
       " 0]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
